{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "colab": {
      "name": "semeval_bert_run1_download.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nazmus007/transformers_test/blob/main/semeval_bert_run1_download.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTQvMtWsztpa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa46643-9945-4967-db7b-a12a238b235e"
      },
      "source": [
        "!pip install -qq transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.9 MB 8.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 80.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 77.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 83.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQls5xsYO8YP",
        "outputId": "ba08b461-9279-4f2f-cd17-a455e2e7f631"
      },
      "source": [
        "print(transformers.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9Jfsp8OsRCw"
      },
      "source": [
        "exit()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8OREO3n6Ft-",
        "outputId": "e7e73474-e905-4b04-b649-a128263792c4"
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.6.0.tar.gz (168 kB)\n",
            "\u001b[?25l\r\u001b[K     |██                              | 10 kB 27.8 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 20 kB 33.1 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 30 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 40 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 51 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 61 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 71 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 81 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 92 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 102 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 112 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 122 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 133 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 143 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 153 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 163 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 168 kB 7.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.0-py3-none-any.whl size=168256 sha256=c03072add7af885432b751b9bef2dbbe29ea774f41a98204b8a093ee8f57e493\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/d7/74/c720aaf345a042b0c2d74361873258c5e8649b7f11b2ccce49\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xreQ6mWHPS1R"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31C71en6z4IP",
        "outputId": "0bd7b019-1ef8-4a2c-c3b0-bdda56ba41a8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FZSJGtBOSvi",
        "outputId": "edc10c13-0d24-4ce4-f48b-4d44d169c492"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Sep  6 09:14:03 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628541801096
        },
        "id": "CO7SFj5Xztpf"
      },
      "source": [
        "import transformers\n",
        "\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup, BertForSequenceClassification\n",
        "\n",
        "import torch\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from pylab import rcParams\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from matplotlib import rc\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from textwrap import wrap\n",
        "\n",
        "from torch import nn, optim\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628541914487
        },
        "id": "p9fpcDXTztpg"
      },
      "source": [
        "import torch.nn.functional as F"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628541916842
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0AbDURbSztph",
        "outputId": "124e766e-a47e-4320-9e63-21318e590dbe"
      },
      "source": [
        "df = pd.read_csv(\"./drive/MyDrive/datasets/cleaned_semeval_wnotebook_5.csv\")\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet index</th>\n",
              "      <th>Label</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>sweet united nations video just in time for ch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>we are rumored to have talked to erv is agent ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>hey there nice to see you minnesotand winter w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3 episodes left i am dying over here</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>i cannot breathe was chosen as the most notabl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Tweet index  Label                                              Tweet\n",
              "0            1      1  sweet united nations video just in time for ch...\n",
              "1            2      1  we are rumored to have talked to erv is agent ...\n",
              "2            3      1  hey there nice to see you minnesotand winter w...\n",
              "3            4      0               3 episodes left i am dying over here\n",
              "4            5      1  i cannot breathe was chosen as the most notabl..."
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628541920084
        },
        "id": "D3arVKEGztpj"
      },
      "source": [
        "df = df.drop(['Tweet index'],axis=1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBY9UMwLovPf"
      },
      "source": [
        "df.dropna(subset = [\"Tweet\"], inplace=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yO2cB6xyo6-L",
        "outputId": "4a6bb812-64c8-446d-975d-e89a580dbccf"
      },
      "source": [
        "!pip install demoji"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting demoji\n",
            "  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▋                        | 10 kB 38.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 20 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 30 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 40 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 42 kB 709 kB/s \n",
            "\u001b[?25hInstalling collected packages: demoji\n",
            "Successfully installed demoji-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK3p30cGo5Cd"
      },
      "source": [
        "import demoji\n",
        "def demoji_text(text):\n",
        "  d= demoji.findall(text)\n",
        "  return d"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuilQ9l_pC87"
      },
      "source": [
        "df['emoji_list'] = df['Tweet'].apply(demoji_text)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awo8cEAWpH8m"
      },
      "source": [
        "f= list(filter(None, df['emoji_list']))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQPVe_5EpLE5",
        "outputId": "b58c7afe-9e9c-4c20-8b7c-05b8e8cb9221"
      },
      "source": [
        "print(f)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'😡': 'pouting face'}, {'😬': 'grimacing face'}, {'❤️': 'red heart'}, {'😅': 'grinning face with sweat'}, {'😁': 'beaming face with smiling eyes'}, {'😐': 'neutral face'}, {'😱': 'face screaming in fear'}, {'👭': 'women holding hands', '☺': 'smiling face'}, {'💯': 'hundred points', '😳': 'flushed face'}, {'😴': 'sleeping face'}, {'💩': 'pile of poo'}, {'😩': 'weary face'}, {'😑': 'expressionless face'}, {'🎶': 'musical notes'}, {'😃': 'grinning face with big eyes', '😇': 'smiling face with halo'}, {'😂': 'face with tears of joy'}, {'🙌': 'raising hands'}, {'😲': 'astonished face', '😂': 'face with tears of joy'}, {'✈️': 'airplane', '❤️': 'red heart', '🌏': 'globe showing Asia-Australia'}, {'😘': 'face blowing a kiss', '😍': 'smiling face with heart-eyes'}, {'😘': 'face blowing a kiss', '😳': 'flushed face', '😍': 'smiling face with heart-eyes'}, {'☀️': 'sun', '🎶': 'musical notes'}, {'🙊': 'speak-no-evil monkey'}, {'😭': 'loudly crying face'}, {'😩': 'weary face'}, {'🌴': 'palm tree', '☀': 'sun'}, {'🔫': 'water pistol', '😀': 'grinning face'}, {'😩': 'weary face'}, {'😆': 'grinning squinting face', '🎅': 'Santa Claus', '🎉': 'party popper', '🎶': 'musical notes'}, {'🍷': 'wine glass'}, {'👎': 'thumbs down', '💨': 'dashing away', '💦': 'sweat droplets', '💧': 'droplet'}, {'😃': 'grinning face with big eyes'}, {'🙈': 'see-no-evil monkey'}, {'😷': 'face with medical mask', '😫': 'tired face'}, {'😆': 'grinning squinting face'}, {'😫': 'tired face'}, {'😂': 'face with tears of joy'}, {'✌️': 'victory hand'}, {'😉': 'winking face'}, {'💅': 'nail polish', '💁': 'person tipping hand', '😁': 'beaming face with smiling eyes', '🙅': 'person gesturing NO'}, {'🎁': 'wrapped gift', '😕': 'confused face'}, {'🙋': 'person raising hand'}, {'☹': 'frowning face'}, {'😔': 'pensive face'}, {'😴': 'sleeping face', '😏': 'smirking face'}, {'😡': 'pouting face'}, {'😡': 'pouting face', '😭': 'loudly crying face'}, {'😂': 'face with tears of joy'}, {'😊': 'smiling face with smiling eyes'}, {'👍': 'thumbs up'}, {'🔫': 'water pistol', '🙅': 'person gesturing NO'}, {'🎁': 'wrapped gift'}, {'👏': 'clapping hands'}, {'😑': 'expressionless face'}, {'😃': 'grinning face with big eyes'}, {'😂': 'face with tears of joy'}, {'😶': 'face without mouth'}, {'👌': 'OK hand'}, {'💘': 'heart with arrow'}, {'🏀': 'basketball', '💯': 'hundred points'}, {'😔': 'pensive face'}, {'😒': 'unamused face'}, {'👍': 'thumbs up', '🎁': 'wrapped gift', '🎅': 'Santa Claus'}, {'😱': 'face screaming in fear'}, {'😜': 'winking face with tongue'}, {'😖': 'confounded face'}, {'😂': 'face with tears of joy', '😍': 'smiling face with heart-eyes'}, {'😑': 'expressionless face', '👎': 'thumbs down'}, {'😄': 'grinning face with smiling eyes'}, {'🚀': 'rocket'}, {'😒': 'unamused face'}, {'✨': 'sparkles'}, {'❤️': 'red heart', '💃': 'woman dancing', '✨': 'sparkles', '👸': 'princess', '💋': 'kiss mark'}, {'😄': 'grinning face with smiling eyes', '😒': 'unamused face'}, {'☔': 'umbrella with rain drops', '🎻': 'violin'}, {'😳': 'flushed face'}, {'😑': 'expressionless face'}, {'❤️': 'red heart', '💁': 'person tipping hand', '😍': 'smiling face with heart-eyes'}, {'😍': 'smiling face with heart-eyes'}, {'✅': 'check mark button'}, {'💋': 'kiss mark', '❤️': 'red heart', '😂': 'face with tears of joy'}, {'🙏': 'folded hands'}, {'🙈': 'see-no-evil monkey', '😭': 'loudly crying face', '😘': 'face blowing a kiss'}, {'😔': 'pensive face'}, {'😛': 'face with tongue'}, {'😒': 'unamused face'}, {'🐶': 'dog face', '🐱': 'cat face'}, {'😩': 'weary face'}, {'😭': 'loudly crying face', '🔪': 'kitchen knife', '🔥': 'fire'}, {'☔': 'umbrella with rain drops', '💦': 'sweat droplets', '💧': 'droplet'}, {'😊': 'smiling face with smiling eyes'}, {'😊': 'smiling face with smiling eyes'}, {'👍': 'thumbs up', '👌': 'OK hand'}, {'😳': 'flushed face'}, {'😁': 'beaming face with smiling eyes', '😳': 'flushed face'}, {'😷': 'face with medical mask'}, {'💜': 'purple heart'}, {'👊': 'oncoming fist'}, {'😐': 'neutral face'}, {'🙈': 'see-no-evil monkey', '😄': 'grinning face with smiling eyes'}, {'😁': 'beaming face with smiling eyes'}, {'😉': 'winking face'}, {'💯': 'hundred points'}, {'😡': 'pouting face', '🙅': 'person gesturing NO'}, {'👳': 'person wearing turban', '😂': 'face with tears of joy'}, {'😘': 'face blowing a kiss'}, {'😁': 'beaming face with smiling eyes', '☺️': 'smiling face'}, {'😭': 'loudly crying face', '🙏': 'folded hands', '☕': 'hot beverage'}, {'⚡': 'high voltage', '☔': 'umbrella with rain drops'}, {'😭': 'loudly crying face'}, {'😣': 'persevering face'}, {'😎': 'smiling face with sunglasses'}, {'😂': 'face with tears of joy'}, {'😕': 'confused face', '😴': 'sleeping face', '💤': 'zzz'}, {'😂': 'face with tears of joy'}, {'😰': 'anxious face with sweat', '😄': 'grinning face with smiling eyes', '😣': 'persevering face', '😖': 'confounded face', '😜': 'winking face with tongue', '😫': 'tired face', '😂': 'face with tears of joy'}, {'👎': 'thumbs down'}, {'😭': 'loudly crying face'}, {'😫': 'tired face', '😍': 'smiling face with heart-eyes'}, {'😒': 'unamused face'}, {'😄': 'grinning face with smiling eyes'}, {'😅': 'grinning face with sweat'}, {'😕': 'confused face'}, {'😂': 'face with tears of joy'}, {'👊': 'oncoming fist', '😒': 'unamused face'}, {'😒': 'unamused face'}, {'💔': 'broken heart', '😖': 'confounded face', '😣': 'persevering face'}, {'😜': 'winking face with tongue'}, {'👍': 'thumbs up'}, {'😁': 'beaming face with smiling eyes', '😊': 'smiling face with smiling eyes', '☺': 'smiling face'}, {'🎄': 'Christmas tree', '☺️': 'smiling face'}, {'😕': 'confused face'}, {'😒': 'unamused face'}, {'🙏': 'folded hands', '😜': 'winking face with tongue'}, {'😤': 'face with steam from nose'}, {'😘': 'face blowing a kiss'}, {'🔫': 'water pistol', '😱': 'face screaming in fear', '🙀': 'weary cat', '🎅': 'Santa Claus', '😂': 'face with tears of joy'}, {'😰': 'anxious face with sweat'}, {'😂': 'face with tears of joy'}, {'🙈': 'see-no-evil monkey'}, {'😖': 'confounded face'}, {'🍕': 'pizza'}, {'😀': 'grinning face'}, {'😁': 'beaming face with smiling eyes', '🔫': 'water pistol', '🍆': 'eggplant'}, {'😆': 'grinning squinting face'}, {'🔫': 'water pistol', '💾': 'floppy disk', '💻': 'laptop'}, {'👌': 'OK hand'}, {'☔': 'umbrella with rain drops'}, {'😋': 'face savoring food'}, {'😂': 'face with tears of joy'}, {'🇫🇴': 'flag: Faroe Islands'}, {'❤️': 'red heart'}, {'💋': 'kiss mark'}, {'👍': 'thumbs up'}, {'😩': 'weary face'}, {'😴': 'sleeping face', '🆘': 'SOS button'}, {'💭': 'thought balloon'}, {'😡': 'pouting face', '😤': 'face with steam from nose', '😠': 'angry face'}, {'☺️': 'smiling face'}, {'😎': 'smiling face with sunglasses'}, {'😡': 'pouting face'}, {'😂': 'face with tears of joy'}, {'💪': 'flexed biceps', '👊': 'oncoming fist'}, {'😴': 'sleeping face'}, {'😣': 'persevering face'}, {'❤️': 'red heart', '😜': 'winking face with tongue'}, {'👍': 'thumbs up', '😒': 'unamused face'}, {'⚾': 'baseball', '😄': 'grinning face with smiling eyes', '🏆': 'trophy', '🙌': 'raising hands'}, {'👎': 'thumbs down'}, {'😂': 'face with tears of joy'}, {'😡': 'pouting face', '🔫': 'water pistol'}, {'😏': 'smirking face'}, {'❤️': 'red heart', '😻': 'smiling cat with heart-eyes', '😂': 'face with tears of joy', '🎉': 'party popper'}, {'💪': 'flexed biceps'}, {'😀': 'grinning face'}, {'👌': 'OK hand'}, {'👌': 'OK hand', '🙈': 'see-no-evil monkey'}, {'😘': 'face blowing a kiss', '❤': 'red heart', '😍': 'smiling face with heart-eyes'}, {'😒': 'unamused face'}, {'😭': 'loudly crying face', '💁': 'person tipping hand', '😂': 'face with tears of joy'}, {'🎵': 'musical note', '💕': 'two hearts'}, {'😴': 'sleeping face', '💤': 'zzz'}, {'🎤': 'microphone'}, {'😝': 'squinting face with tongue'}, {'💀': 'skull'}, {'🙊': 'speak-no-evil monkey', '😁': 'beaming face with smiling eyes', '🎉': 'party popper'}, {'😅': 'grinning face with sweat'}, {'👍': 'thumbs up', '🛀': 'person taking bath', '💚': 'green heart'}, {'😢': 'crying face'}, {'😒': 'unamused face'}, {'😑': 'expressionless face'}, {'💕': 'two hearts'}, {'🐸': 'frog', '☕': 'hot beverage'}, {'🙈': 'see-no-evil monkey'}, {'👍': 'thumbs up', '😒': 'unamused face'}, {'😍': 'smiling face with heart-eyes'}, {'😝': 'squinting face with tongue', '❤️': 'red heart', '💚': 'green heart', '😍': 'smiling face with heart-eyes'}, {'🙊': 'speak-no-evil monkey'}, {'🐸': 'frog', '☕': 'hot beverage'}, {'👊': 'oncoming fist'}, {'😒': 'unamused face', '😔': 'pensive face'}, {'😎': 'smiling face with sunglasses'}, {'🙈': 'see-no-evil monkey', '😂': 'face with tears of joy', '😳': 'flushed face'}, {'👌': 'OK hand'}, {'😒': 'unamused face'}, {'💜': 'purple heart'}, {'😂': 'face with tears of joy'}, {'😭': 'loudly crying face', '😂': 'face with tears of joy'}, {'😆': 'grinning squinting face', '😎': 'smiling face with sunglasses'}, {'😏': 'smirking face'}, {'🎄': 'Christmas tree', '🎅': 'Santa Claus'}, {'🎄': 'Christmas tree', '😊': 'smiling face with smiling eyes', '😤': 'face with steam from nose'}, {'😩': 'weary face'}, {'🙈': 'see-no-evil monkey', '😂': 'face with tears of joy'}, {'😡': 'pouting face', '😑': 'expressionless face'}, {'😂': 'face with tears of joy'}, {'😒': 'unamused face'}, {'😳': 'flushed face'}, {'✈️': 'airplane'}, {'😩': 'weary face'}, {'💋': 'kiss mark'}, {'😁': 'beaming face with smiling eyes'}, {'👊': 'oncoming fist', '😂': 'face with tears of joy'}, {'😂': 'face with tears of joy'}, {'😊': 'smiling face with smiling eyes'}, {'😂': 'face with tears of joy'}, {'😂': 'face with tears of joy'}, {'🙊': 'speak-no-evil monkey'}, {'👍': 'thumbs up', '🎁': 'wrapped gift', '🎅': 'Santa Claus'}, {'🎧': 'headphone'}, {'😂': 'face with tears of joy'}, {'🎄': 'Christmas tree', '😏': 'smirking face', '😎': 'smiling face with sunglasses'}, {'🐁': 'mouse'}, {'💁': 'person tipping hand'}, {'👍': 'thumbs up', '😃': 'grinning face with big eyes'}, {'🙈': 'see-no-evil monkey', '💕': 'two hearts'}, {'😷': 'face with medical mask'}, {'☁': 'cloud'}, {'😒': 'unamused face'}, {'😣': 'persevering face'}, {'😬': 'grimacing face'}, {'🚓': 'police car', '💀': 'skull'}, {'🙆': 'person gesturing OK'}, {'💣': 'bomb'}, {'😂': 'face with tears of joy'}, {'🍹': 'tropical drink'}, {'😏': 'smirking face'}, {'😂': 'face with tears of joy'}, {'😷': 'face with medical mask', '😒': 'unamused face'}, {'🙈': 'see-no-evil monkey', '🙊': 'speak-no-evil monkey', '😂': 'face with tears of joy'}, {'😝': 'squinting face with tongue'}, {'😊': 'smiling face with smiling eyes'}, {'😞': 'disappointed face', '😔': 'pensive face'}, {'👑': 'crown', '😏': 'smirking face', '🙅': 'person gesturing NO'}, {'😂': 'face with tears of joy'}, {'😜': 'winking face with tongue'}, {'😳': 'flushed face'}, {'😂': 'face with tears of joy'}, {'👍': 'thumbs up'}, {'😘': 'face blowing a kiss'}, {'😁': 'beaming face with smiling eyes'}, {'😑': 'expressionless face', '😒': 'unamused face'}, {'🔪': 'kitchen knife', '😵': 'knocked-out face'}, {'😒': 'unamused face'}, {'👊': 'oncoming fist', '🎼': 'musical score', '👏': 'clapping hands', '😎': 'smiling face with sunglasses'}, {'😊': 'smiling face with smiling eyes', '🎶': 'musical notes'}, {'🚶': 'person walking'}, {'🔌': 'electric plug'}, {'❄️': 'snowflake', '🇬🇧': 'flag: United Kingdom'}, {'😃': 'grinning face with big eyes', '😂': 'face with tears of joy'}, {'😭': 'loudly crying face'}, {'😂': 'face with tears of joy'}, {'👌': 'OK hand'}, {'😏': 'smirking face'}, {'👊': 'oncoming fist', '😏': 'smirking face'}, {'♥': 'heart suit'}, {'😒': 'unamused face'}, {'😡': 'pouting face'}, {'🍸': 'cocktail glass', '🔫': 'water pistol', '☺': 'smiling face'}, {'😏': 'smirking face', '😳': 'flushed face'}, {'💃': 'woman dancing', '📖': 'open book', '👯': 'people with bunny ears', '📚': 'books'}, {'💖': 'sparkling heart', '😇': 'smiling face with halo'}, {'😂': 'face with tears of joy'}, {'😊': 'smiling face with smiling eyes'}, {'😔': 'pensive face'}, {'😲': 'astonished face', '🔫': 'water pistol'}, {'💜': 'purple heart'}, {'👍': 'thumbs up', '🙌': 'raising hands', '👏': 'clapping hands'}, {'😘': 'face blowing a kiss'}, {'☺': 'smiling face'}, {'👍': 'thumbs up'}, {'🔜': 'SOON arrow', '📷': 'camera', '➡️': 'right arrow'}, {'💜': 'purple heart', '☺': 'smiling face'}, {'😂': 'face with tears of joy'}, {'💕': 'two hearts'}, {'🙌': 'raising hands'}, {'❤️': 'red heart'}, {'💁': 'person tipping hand', '😊': 'smiling face with smiling eyes'}, {'😘': 'face blowing a kiss', '💕': 'two hearts', '🙌': 'raising hands'}, {'😒': 'unamused face'}, {'💕': 'two hearts', '💜': 'purple heart'}, {'🍵': 'teacup without handle'}, {'🔜': 'SOON arrow', '🔥': 'fire'}, {'🎉': 'party popper', '☺': 'smiling face'}, {'😩': 'weary face', '🙈': 'see-no-evil monkey', '😂': 'face with tears of joy'}, {'😒': 'unamused face'}, {'😂': 'face with tears of joy'}, {'😞': 'disappointed face', '😕': 'confused face', '😟': 'worried face', '😢': 'crying face', '😪': 'sleepy face'}, {'😂': 'face with tears of joy'}, {'🔥': 'fire'}, {'👍': 'thumbs up', '😍': 'smiling face with heart-eyes'}, {'😉': 'winking face', '🍻': 'clinking beer mugs', '😂': 'face with tears of joy'}, {'😃': 'grinning face with big eyes'}, {'😂': 'face with tears of joy'}, {'😭': 'loudly crying face', '😢': 'crying face'}, {'😅': 'grinning face with sweat'}, {'😁': 'beaming face with smiling eyes', '😘': 'face blowing a kiss'}, {'😘': 'face blowing a kiss'}, {'😁': 'beaming face with smiling eyes'}, {'☺': 'smiling face'}, {'😍': 'smiling face with heart-eyes'}, {'🐣': 'hatching chick', '😂': 'face with tears of joy'}, {'🙈': 'see-no-evil monkey'}, {'😍': 'smiling face with heart-eyes'}, {'😂': 'face with tears of joy'}, {'💁': 'person tipping hand'}, {'✌️': 'victory hand'}, {'😂': 'face with tears of joy'}, {'😂': 'face with tears of joy', '😋': 'face savoring food'}, {'🏃': 'person running', '😑': 'expressionless face'}, {'😂': 'face with tears of joy'}, {'😅': 'grinning face with sweat', '😱': 'face screaming in fear'}, {'👐': 'open hands', '😂': 'face with tears of joy'}, {'💜': 'purple heart', '💕': 'two hearts'}, {'💓': 'beating heart'}, {'👊': 'oncoming fist', '😂': 'face with tears of joy'}, {'😑': 'expressionless face'}, {'😂': 'face with tears of joy', '😜': 'winking face with tongue'}, {'💜': 'purple heart'}, {'😳': 'flushed face'}, {'🙌': 'raising hands', '😂': 'face with tears of joy', '✌️': 'victory hand'}, {'😡': 'pouting face'}, {'☺': 'smiling face'}, {'💪': 'flexed biceps'}, {'😁': 'beaming face with smiling eyes'}, {'😒': 'unamused face'}, {'😣': 'persevering face'}, {'🎄': 'Christmas tree', '🎅': 'Santa Claus'}, {'😒': 'unamused face'}, {'💚': 'green heart'}, {'🎄': 'Christmas tree', '🎁': 'wrapped gift', '🎅': 'Santa Claus'}, {'😂': 'face with tears of joy'}, {'😂': 'face with tears of joy'}, {'😒': 'unamused face'}, {'😂': 'face with tears of joy'}, {'😩': 'weary face', '😔': 'pensive face'}, {'🙈': 'see-no-evil monkey'}, {'😕': 'confused face'}, {'😁': 'beaming face with smiling eyes'}, {'😩': 'weary face'}, {'👍': 'thumbs up', '👎': 'thumbs down'}, {'❤️': 'red heart'}, {'💗': 'growing heart', '🔫': 'water pistol', '😞': 'disappointed face'}, {'🔫': 'water pistol', '😒': 'unamused face'}, {'🎉': 'party popper'}, {'❄️': 'snowflake'}, {'😜': 'winking face with tongue'}, {'😁': 'beaming face with smiling eyes'}, {'😉': 'winking face'}, {'🍴': 'fork and knife'}, {'🍟': 'french fries'}, {'🙊': 'speak-no-evil monkey', '😂': 'face with tears of joy'}, {'😘': 'face blowing a kiss', '🎅': 'Santa Claus', '🎉': 'party popper', '🎶': 'musical notes'}, {'💃': 'woman dancing', '✋': 'raised hand'}, {'👌': 'OK hand'}, {'✌️': 'victory hand'}, {'🚍': 'oncoming bus', '⛄': 'snowman without snow', '❄': 'snowflake', '😋': 'face savoring food'}, {'😳': 'flushed face'}, {'🙊': 'speak-no-evil monkey', '👯': 'people with bunny ears', '📚': 'books'}, {'😒': 'unamused face'}, {'😴': 'sleeping face', '🚑': 'ambulance', '🚒': 'fire engine'}, {'👍': 'thumbs up', '😃': 'grinning face with big eyes'}, {'😖': 'confounded face'}, {'🎄': 'Christmas tree', '✈️': 'airplane', '❤️': 'red heart'}, {'😂': 'face with tears of joy'}, {'💰': 'money bag'}, {'😴': 'sleeping face'}, {'✨': 'sparkles', '🙇': 'person bowing'}, {'😒': 'unamused face'}, {'😆': 'grinning squinting face', '☺️': 'smiling face', '👌': 'OK hand', '🙈': 'see-no-evil monkey'}, {'👉': 'backhand index pointing right', '🙌': 'raising hands', '💜': 'purple heart', '⭕': 'hollow red circle', '❌': 'cross mark', '🙏': 'folded hands', '👏': 'clapping hands', '👈': 'backhand index pointing left'}, {'😜': 'winking face with tongue'}, {'💕': 'two hearts', '😬': 'grimacing face', '💜': 'purple heart'}, {'😖': 'confounded face', '✌️': 'victory hand', '😔': 'pensive face'}, {'😣': 'persevering face', '😷': 'face with medical mask'}, {'😁': 'beaming face with smiling eyes'}, {'😎': 'smiling face with sunglasses'}, {'😂': 'face with tears of joy'}, {'💅': 'nail polish'}, {'😔': 'pensive face'}, {'👤': 'bust in silhouette'}, {'💩': 'pile of poo'}, {'👑': 'crown'}, {'👌': 'OK hand'}, {'😳': 'flushed face'}, {'😂': 'face with tears of joy'}, {'💀': 'skull', '😂': 'face with tears of joy'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_G9uMqxpNLS",
        "outputId": "1cb2f87b-a67f-49a9-e68a-57d18e96c337"
      },
      "source": [
        "any(f)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgAAOOMnpSkD"
      },
      "source": [
        "all_keys = set().union(*(d.keys() for d in f))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0wxgTLepW0U",
        "outputId": "eca132f1-2d7b-4fd4-a924-032fe3c23bcf"
      },
      "source": [
        "print(all_keys)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'👳', '🚑', '⛄', '❄', '🚒', '🆘', '😱', '😛', '❌', '👯', '💧', '💃', '😁', '🐁', '🍵', '😃', '😬', '❤', '🙏', '😠', '😂', '👎', '😰', '✈️', '😊', '😏', '👌', '🚀', '🎵', '💤', '👍', '👊', '💜', '🔥', '😅', '🎧', '😜', '🎁', '🎼', '🍴', '📚', '🙈', '🏃', '🔌', '😀', '💻', '😎', '🍸', '🙋', '✌️', '💾', '😖', '🔪', '🏆', '🌏', '✋', '💁', '😫', '😋', '👈', '🍆', '☀', '☁', '☺️', '🐱', '💅', '⚾', '🎤', '♥', '🌴', '🐶', '🙌', '💕', '😘', '🍹', '💩', '💣', '😐', '🙆', '🍻', '😷', '🏀', '😑', '🍟', '💰', '🐸', '💘', '😟', '😳', '💋', '🚓', '😪', '😒', '💯', '😍', '😩', '💗', '☹', '⭕', '😲', '🐣', '🚍', '👏', '🎻', '😭', '💪', '😻', '💀', '🍷', '🛀', '❄️', '📖', '👸', '☀️', '🙊', '😄', '🔫', '😇', '🙀', '🎄', '😶', '💓', '✨', '⚡', '➡️', '🙅', '😕', '💨', '😤', '😔', '👐', '☕', '😞', '😵', '😣', '🇫🇴', '💦', '😡', '✅', '🇬🇧', '🙇', '👑', '☔', '😢', '🎶', '💔', '💚', '😆', '😝', '🚶', '💖', '🎉', '💭', '👤', '🎅', '❤️', '😉', '🍕', '👉', '☺', '😴', '👭', '🔜', '📷'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOtRJofY3iNX"
      },
      "source": [
        "emoji_1 = re.compile('[\\\\u203C-\\\\u3299\\\\U0001F000-\\\\U0001F644]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0xz4FhI5Vyt"
      },
      "source": [
        "emoji_list= list(filter(emoji_1.match, df['Tweet']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3xabioA5e-Y",
        "outputId": "87fc3ddc-207a-4e3c-e09d-b266de21aed9"
      },
      "source": [
        "print(emoji_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['🍃oh this will surely help the admissions office with their recruitment of students of color    ', '😏 i enjoy our random conversations at the bcm and i thank you for introducing me to one direction  godawgs', '😙love kpop i love kpop dude  pestimistic evil beautiful coldandcalculating lanadelrey artpop f4f shoutout', '💎👑💎👑💎👑1k tweet goes to  because shes kallie and she shes sassy and gets what she wants when she wants it 👑💎👑💎👑💎  😘', '😱 rt  rt  yeah gsp definitely won that fight  ', '😂 rt   my uncle used to refer to missed calls as miskeen calls  laugh out loud', '😂 “ awwwwwww saigon got some drake in him how adorable   lhhny”', '😕 looks like i am not goin to sleep shower there ', '★ all my best wishes andamp happy new y34r 2014 keep partying andamp rave on  2013 was greatfull of goodiesand ', '☕️good morning from germany         🍩', '💊 a study in pink 🎎 the blind banker 💣 the great game  📱 a scandal in belgravia 🐾 the hounds of baskerville 🏥 the reichenbach fall', '♡ me myself someoneelse mystyle jogger jogginghose kaputze ♡ ', '★ transmission  the machine of transformation ★ countdown  16 days  ► ', '💕thanks        happyfriday all💕', '🌏 the world is a book and those who do not travel read only a page 🌍', '【europe】 2013126 231446 utc9  location 146km ne of praguecz  m31 tnt6736kg  depth 20km', '❤ ♫ lola stars and stripes by the stills at dianti catering company — ', '♫ just announced nepomuk czech republic  dec 27 at tlustá berta ', '🌞h ꭺꮲꮲꮍwꭼꭼꮶꭼꮑꭰ mꮍ fꮢꭵꭼꮑꭰs💕        ', '♫ just downloaded above andamp beyond  sun andamp moon drop by     via ', '★★just on the gdjb with markus schulz united music with transmission monday 2511 exclusive on beatport thanks ', '😆“ i bet andy reid is looking at the waffle house menu”', '♫ today kladno czech republic  nov 30 at bpm club ', '♫ just announced kladno czech republic  feb 22 at bpm ', '🎧my top 3 lastfm artists obituary 13 tiamat 12 andamp the toy dolls 11 tweeklyfm ', '👌👌💯  your so cuteee laugh out loud buuuut you got fake and sopped talkin to me 😒', '☕️goodmorning         happywednesday 🌞', '♫ just announced villach austria  jan 31 at vclub ', '☹ ☹  i do not understand why people still eat mcdonald is ☹ ☹', '★★all the way from buenos aires argentina tomas heredia you will hear a lot more about him★★ ', '🎧my top 3 lastfm artists slayer 12 tool 9 andamp serj tankian 7 ', '♫ today altenhof austria  nov 30 at karibikparty ', '🎄time to stick together💝xmas czech xmasmood  mírové náměstí ', '♫ just downloaded lorde  no better dropwizz festi by     via ', '★steve helstrip as known as the thrillseekers is asking which tracks you would like to hear the night before on trance ', '😍😍 with theo in berlin 10112013 😘❤️  hurtsfamily hurts ', '♫ just downloaded new world sound andamp thomas newson  by     via ', 'ㅠㅠ ㅠㅠ rt  ahh panda hyung do not cry tt cr kana ', '🎧my top 3 lastfm artists grimes 12 bosnian rainbows 11 andamp bonobo 10 ', '★ šok ★ hardwell priznal že ghostprodukoval 1 track z beatport top 10 ktorý to asi bude  s touto temnou ', '▶ building mobile apps using salesforce geolocation features  youtube ', '🎧my top 3 lastfm artists ryan leslie 22 vec 4 andamp mariah carey 2 ', '♫ just announced mlada boleslav czech republic  mar 14 at forum ', '▶ salesforce dataloader explained in 30 minute  youtube ', '♥♥♥ rt  make sure to watch the special stage of exo 12월의기적miracles in december at kbs music bank ', '😩☺️  oh my god my favorite memory with you was at the track meet when you were acting craaaazy hahaha i miss you', '🎧my top 3 lastfm artists nmzs 16 olli banjo 7 andamp kontra k 7 music ', '♡♡♡jundyununeo 131216 samsung medical centre     ', '🎶🎶 there is the girl that i like🎶🎶 hardstyle qlimax girls want to meet her  lost in paradise ', '★★cosmic gate rocked the o2 arena hard last saturday★★ ', '♫ today hradec kralove czech republic  dec 13 at ozzies ', '▶ guy kawasaki how to use social media as an evangelist for your business and here is how i did it youtube ', '♫ just downloaded borgore andamp victor niglio  booty m by     via ', '【europe】 20131129 224156 utc9  location 69km e of praguecz  ml25 tnt848kg  depth 20km', '😍 rt  from last night mr synyster gates doing what he does best 😝😊  ', '🍩good morning from germany☕️        happy', '★just played on a state of trance official by armin van buuren transmission  the machine of transformation by ', '♫ just announced melbourne australia  mar 7 at the reverence hotel  the corner hotel ', '♡  junbros  cras tagged charityevent samsungmedicalcentre ', '💕thanks salim my friend       happyfriday all my friends💕', '♫ just announced innsbruck austria  dec 20 at queens club ', '♫ mortal kombat theme  game theme – misterioustheme  nowplaying', '🐉  your really chill but we do not talk anymore and i might possibly be your cheerleader 😳😛', '💗bebe peludo 🐶🐶 puppy love lennon schnauzer happiness ', '🎄my top 3 lastfm artists the irish rovers 6 carach angren 5 andamp vreid 3 ', '🎧my top 3 lastfm artists carnifex 11 blind ambitions 8 andamp hatebreed 3 lastfm ', '🐸🐸🐸 with arif and ikbal at royal stag bistro pic — ', '🌞goodmorning ☕️       followfriday', '⭐️⭐️⭐️  gold followme czech rapamoda black iphoneonly vscocam  jiřího hood ', '◘ promo time ◘  ▬ promo me first ▬ promo rpattzandampkrisstew ▬ comment done ▬ we will do back promo ', '♫ just downloaded martin garrix  animals dropwizz by     via ', '₪ ø lll ·o  czech echelon loves you czechrepublicneedsmars   ₪ ø lll ·o', '🐒your so pretty youandampyour boyfriend are so cute together we used to be close friends i do not know what happened ', '♫ today prague czech republic  dec 31 at nye  cross club ', '♫ just announced simbach germany  jan 18 at rezolut ', '🎧my top 3 lastfm artists vector lovers 14 conforce 10 andamp the heliocentrics 8 ', '💕f0ll0wthebest💕       ', '【europe】 2013126 231449 utc9  location 129km ne of praguecz  ml31 tnt6736kg  depth 100km', '【europe】 2013125 193439 utc9  location 91km nnw of wienat  ml27 tnt1692kg  depth 20km', '🎧my top 2 lastfm artists the toy dolls 51 andamp jo nesbö  nemesis 1 ', '🎧my top 3 lastfm artists chvrches 50 oddisee 24 andamp anoraak 14  ', '♫ today nymburk czech republic  dec 25 at celebrity night ', '♫ just announced banbridge united kingdom  feb 8 at marc van gale pres digital collective 300 ', '【europe】 20131217 35541 utc9  location 95km w of dresdende  me too1 tnt213kg  depth 100km', '★giveaway★win the spiral of bliss trilogy by   at  today giveaway  ', '☕️good morning from germany🍩    happyweekend my friends💕', '♫ just downloaded showtek  we like to party dropw by     via ', '♫ changing of the seasons – two door cinema club  nowplaying', '♫ today prague czech republic  dec 12 at rc kain ', '②ality – javascript and more web platform five technologies to look forward to ', '★ ★ jste na tohle připraveni ★ ★ are you ready for this  ➥ vstupenky čr  ➥ tickets ', '😢 rt  someone come kidnap me so i do not have to leave', '💚 rt  look who is here come and say hello to yumi and her cute little baby boy in our berlin boutique ', '🌞gm☕️         followfriday', 'プラハなう  今日の１面はベルルスコーニw  letiště václava havla  václav havel airport prg w 7 others pic ', '★ soutěž o 3 kiss 2 volné vstupy na páteční ministry of sound cz party s moguaiem v duplex prague czech republic ★ ', '【europe】 2013126 231446 utc9  location 149km ne of praguecz  m31 tnt6736kg  depth 80km', '🎧my top 3 lastfm artists gossip 13 haggard 6 andamp madrid 2 kascalfoni ', '→ one guy with a marker just made the global warming debate completely obsolete via  ', '🎧my top 3 lastfm artists darkthrone 65 cannibal corpse 11 andamp excrementory grindfuckers 9 tweeklyfm ', '♫ just announced plauen germany  dec 21 at malzhaus ', '😔 it is been 5 yrs since i have been home for a thanksgiving dinneri did not even realize thanksgiving had pasted the life of a athlete', '♫♫♫ new mashup release ♫♫♫  this is my last mashup in 2013 have fun  ', '♫ today ostrava czech republic  nov 23 at plan b hardcore cafe ', '😍 vacation church  katedrála sv víta  saint vitus cathedral ', '🎧my top 3 lastfm artists parov stelar 28 ordo rosarius equilibrio 26 andamp miranda sex garden 25 lastfm ', '🍀happy new year my friends🍀        ', '😍 rt  today i got a glimpse of heaven in the form of a light skin male with crystal blue eyes', '🎄🎅🎄🎁🎄❄️🎄⛄️🎄 prague city walk czechrepublic bethlehem chapel historic centre old wood… ', '♫ animals  radio edit – martin garrix  nowplaying', '♪no one can find the rewind button now sing it if you understand ♪ no problem', '🎧my top 3 lastfm artists miley cyrus 6 kai tracid 4 andamp chicane 2 ', '♫ just downloaded laidback luke andamp peking duk  mufa by     via ', '♫ today cottbus germany  nov 30 at blue moon festival ', 'ㅋㅋ rt  laugh out loud rt  pic 131122 honkong airport  sleepy sunggyu crstaygyu ', '♫ just announced osoppo italy  aug 9 at pietrasonica ', '💕hello my dear friends 💕    💕', '😊😍❤ millaapplea delicious cheerleader love fun pink proud ', '🍊🏈🍻you are so tall and really country i saw you at union city wal mart😂', '♫ a thousand years – christina perri  nowplaying', '【ml30】poland depth 10km nov 16 2013 121240 utc grin emsc', '✨ merry christmas to all my ig and twitter friends ✨ ', '★ all my best wishes andamp happy new y34r 2014 keep partying andamp rave on  2013 was greatfull of goodiesand ', '♫ today ceska lipa czech republic  dec 24 at luxor ftc ', '♫ just announced villach austria  jan 25 at vclub ', '♫ access all areas remixed andamp bside – atomic kitten  nowplaying', '♥♥♥ rt  gif 131122 yixing heart to camera cr ', '► neues produkt plattenspieler  hands on by dj stefan  ', '▶ cannabinoids  antiproliferative antiangiogenic antimetastatic ', '♫ heart of the city is not no love – jay z  nowplaying', '♫ safe andamp sound  from the hunger games soundtrack – taylor swiftthe civil wars  nowplaying', '♫ today prague czech republic  nov 30 at transmission 10th anniversary ', '【europe】 20131221 234731 utc9  location 137km ne of praguecz  ml32 tnt9515kg  depth 100km', '🎧my top 3 lastfm artists howard shore 14 dżem 8 andamp justin timberlake 7 ', '☹ i miss him just i am remembering the last year for this date  it was wonderful  with you ', '♫ just announced thesau germany  dec 14 at club ', '♫ today mons belgium  dec 20 at lotto mons club ', '👫🎈❤ love tagsforlikes tflers tweegram photooftheday 20likes amazing followme follow4follow… ', '♫ just downloaded bingo players  mode dropwizz tr by     via ', '♫ just announced velke pavlovice czech republic  jan 25 at millenium ', '♫ today brno czech republic  dec 13 at 7 nebe ', '★★always one of the highlights at transmission the transmix 1520 minutes full of trance classics lasers ', '【europe】 20131219 205953 utc9  location 61km w of praguecz  ml19 tnt107kg  depth 20km', '♫ just downloaded daft punk  get lucky dropwizz c by     via ', '▶  november transactions better than expected says piper jaffray29371291  youtube ', '😋 deli salat feldsalat jogurtsauce healty vegetarian whatvegetarianseat schnittlauch… ', '♫ today prague czech republic  nov 18 at roxy ', '🎀i honestly think we would be great friends we think alike and you getting humor  your so pretty too😍', '♫ just downloaded i am the melodyman hardcore edit by     via ', '♫ just announced hranice budweis czech republic  jan 25 at city bar ', '♫ africa version longue – rose laurens  nowplaying', '🍀happy new year my friends🍀       ff', '😇 solid as a mf rock condolences goes out to ma real niqqa 🙉🙈🙊 rainshower 💦👼 ', '★yeah one of the best locations ever  thanks rotterdam  thanks maassilo for the great night ', '🍩goodmorning my friends☕️   ', '💕thank you hug for canada💕      😘', '► news tomorrowland cz  sk sa rozširuje na 2 víkendy  ► en tomorrowland 2014 expands to 2 weekends ', '🎄🎈 merry christmas from the thomas johnson is andamp the huggins xoxo ', '♫ today plzen czech republic  dec 7 at club 28 ', '♩♬♪rain drops keep fallin on my head♩♪♬  galfa gmbh andamp co kg pic ', '♥  goodmorning hottests have a good day today ♥ ', '★★first set online from last saturdays transmission  the machine of transformation enjoy once more thomas ', 'ㅋㅋㅋㅋㅋㅋㅋ 이게 모얔ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ  rt  manly taecyeon  okcat incredible taec ', '♫ today pardubice czech republic  dec 27 at the big bang ', '♫ just downloaded ellie goulding  you my everythi by     via ', '☕️good morning from germany🌞         happyday', '♫ just announced ceske budejovice czech republic  may 31 at budvar ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev1FyCD87SxC"
      },
      "source": [
        "emo_found= ' '.join(emoji for emoji in emoji_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8GjMz7F9Otc"
      },
      "source": [
        "from emoji import UNICODE_EMOJI\n",
        "\n",
        "def get_emoji_set(text):\n",
        "    return {letter for letter in text if letter in UNICODE_EMOJI['en'] }"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBt3k70f930z"
      },
      "source": [
        "c = get_emoji_set(emo_found)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgL3XTIDCAWV",
        "outputId": "447611d1-b4a0-4994-ec9b-a5778212d8e0"
      },
      "source": [
        "print(c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'😕', '😳', '❤', '🎁', '😢', '🎈', '📱', '🐶', '💎', '🎶', '😂', '🌞', '🎧', '🍩', '😩', '🐒', '🍀', '😍', '🌏', '😊', '💗', '🍻', '😋', '🎀', '😇', '🏈', '🙊', '☺', '⭐', '😱', '😏', '👼', '🌍', '💦', '👫', '🎅', '🍊', '👑', '🙈', '💊', '💣', '🐾', '🎎', '😆', '👌', '😙', '💯', '🎄', '🏥', '▶', '🐉', '😛', '💚', '😔', '😒', '❄', '☹', '♥', '✨', '🙉', '💕', '🐸', '💝', '😝', '⛄', '☕', '🍃', '😘'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628541925058
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "T7udEFkrztpl",
        "outputId": "1ca076f0-00ea-430e-8a01-7207a3302448"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>peyton obviously cannot finish games he just t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>well i would love to be sleeping right now but...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>my biggest executive decision today choosing b...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the first quality opponent they played in 2 y...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>but do not you dare call it hate crimes  libe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Tweet  Label\n",
              "0  peyton obviously cannot finish games he just t...      1\n",
              "1  well i would love to be sleeping right now but...      1\n",
              "2  my biggest executive decision today choosing b...      1\n",
              "3   the first quality opponent they played in 2 y...      1\n",
              "4   but do not you dare call it hate crimes  libe...      1"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzzBpq37AC7u",
        "outputId": "e5c2466c-f9f7-473d-c8fb-d09368f2ed93"
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.5.0.tar.gz (185 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▊                              | 10 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 20 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 30 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████                         | 40 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 61 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 71 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 81 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 92 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 102 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 112 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 122 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 133 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 143 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 153 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 163 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 174 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 184 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 185 kB 4.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.5.0-py3-none-any.whl size=187457 sha256=10e21c28e7cb748fa9d5bfa6b2cb229532ca7be45c698195f08b607de61ddb65\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/b5/f6/b39abf14e94b3d6640613bbe630a66c10ccf7a12882d064fb5\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV3PGj42pg94"
      },
      "source": [
        "import emoji\n",
        "def extract_emojis(s):\n",
        "    return ''.join((' '+c+' ') if c in emoji.UNICODE_EMOJI['en'] else c for c in s)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxFwc-UMpm3y"
      },
      "source": [
        "df['Tweet'] = df['Tweet'].apply(extract_emojis)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpZhay9npuCa"
      },
      "source": [
        "df.columns = df.columns.str.lstrip()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "UkoC2K-Vp1MM",
        "outputId": "d2527fc6-3ed5-4111-ba77-1e393e8fd842"
      },
      "source": [
        "rcParams['figure.figsize'] = 8, 6\n",
        "sns.countplot(df.Label)\n",
        "plt.xlabel('class comparison');"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYqXhxJ6xwmI"
      },
      "source": [
        "exit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628541966424
        },
        "id": "4PDO5OJ2ztpt"
      },
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628541969740
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMPaF5ANztpt",
        "outputId": "2865ff4e-722f-440f-d3ce-1ac6abb33f80"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "tokenizer.add_tokens(['👳', '🚑', '⛄', '❄', '🚒', '🆘', '😱', '😛', '❌', '👯', '💧', '💃', '😁', '🐁', '🍵', '😃', '😬', '❤', '🙏', '😠', '😂', '👎', '😰', '✈️', '😊', '😏', '👌', '🚀', '🎵', '💤', '👍', '👊', '💜', '🔥', '😅', '🎧', '😜', '🎁', '🎼', '🍴', '📚', '🙈', '🏃', '🔌', '😀', '💻', '😎', '🍸', '🙋', '✌️', '💾', '😖', '🔪', '🏆', '🌏', '✋', '💁', '😫', '😋', '👈', '🍆', '☀', '☁', '☺️', '🐱', '💅', '⚾', '🎤', '♥', '🌴', '🐶', '🙌', '💕', '😘', '🍹', '💩', '💣', '😐', '🙆', '🍻', '😷', '🏀', '😑', '🍟', '💰', '🐸', '💘', '😟', '😳', '💋', '🚓', '😪', '😒', '💯', '😍', '😩', '💗', '☹', '⭕', '😲', '🐣', '🚍', '👏', '🎻', '😭', '💪', '😻', '💀', '🍷', '🛀', '❄️', '📖', '👸', '☀️', '🙊', '😄', '🔫', '😇', '🙀', '🎄', '😶', '💓', '✨', '⚡', '➡️', '🙅', '😕', '💨', '😤', '😔', '👐', '☕', '😞', '😵', '😣', '🇫🇴', '💦', '😡', '✅', '🇬🇧', '🙇', '👑', '☔', '😢', '🎶', '💔', '💚', '😆', '😝', '🚶', '💖', '🎉', '💭', '👤', '🎅', '❤️', '😉', '🍕', '👉', '☺', '😴', '👭', '🔜', '📷'])\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "163"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628541972549
        },
        "id": "CdB1Fa5Bztpt"
      },
      "source": [
        "sample_txt = 'didnt have no time to wake and bake this mornin  winnin 😡 🙅'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTIxAVjrBbCc"
      },
      "source": [
        "exit()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628541975382
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KRki6qaztpu",
        "outputId": "023be6b8-c4f1-42a8-9afa-0ec5931ac953"
      },
      "source": [
        "tokens = tokenizer.tokenize(sample_txt)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(f' Sentence: {sample_txt}')\n",
        "print(f'   Tokens: {tokens}')\n",
        "print(f'Token IDs: {token_ids}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Sentence: didnt have no time to wake and bake this mornin  winnin 😡 🙅\n",
            "   Tokens: ['didn', '##t', 'have', 'no', 'time', 'to', 'wake', 'and', 'ba', '##ke', 'this', 'mor', '##nin', 'win', '##nin', '😡', '🙅']\n",
            "Token IDs: [2134, 2102, 2031, 2053, 2051, 2000, 5256, 1998, 8670, 3489, 2023, 22822, 11483, 2663, 11483, 30658, 30646]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628541980729
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TGOBaXEztpw",
        "outputId": "1536b966-08fa-499b-8460-0f054cac8b5a"
      },
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "  sample_txt,\n",
        "  max_length=32,\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628541984214
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnIUMAqDztpx",
        "outputId": "4046a959-4a4f-468c-dcc4-cc03f72f8303"
      },
      "source": [
        "print(len(encoding['input_ids'][0]))\n",
        "encoding['input_ids'][0]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  101,  2134,  2102,  2031,  2053,  2051,  2000,  5256,  1998,  8670,\n",
              "         3489,  2023, 22822, 11483,  2663, 11483, 30658, 30646,   102,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628541986847
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG59VX9Jztpy",
        "outputId": "c3340901-b20b-494f-9c47-3aeb0a582467"
      },
      "source": [
        "print(len(encoding['attention_mask'][0]))\n",
        "encoding['attention_mask']"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628541989596
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u_nUjq-ztpy",
        "outputId": "b9150f46-db8a-475c-dcf3-c3e26beaf13f"
      },
      "source": [
        "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'didn',\n",
              " '##t',\n",
              " 'have',\n",
              " 'no',\n",
              " 'time',\n",
              " 'to',\n",
              " 'wake',\n",
              " 'and',\n",
              " 'ba',\n",
              " '##ke',\n",
              " 'this',\n",
              " 'mor',\n",
              " '##nin',\n",
              " 'win',\n",
              " '##nin',\n",
              " '😡',\n",
              " '🙅',\n",
              " '[SEP]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628541992915
        },
        "id": "mZDQdBGXztpz"
      },
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in df.Tweet:\n",
        "  tokens = tokenizer.encode(txt, max_length=256)\n",
        "  token_lens.append(len(tokens))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628541994652
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "4TcEmypFztp0",
        "outputId": "1a109478-0eeb-4e30-9052-1b3c9dccef0f"
      },
      "source": [
        "sns.distplot(token_lens)\n",
        "plt.xlim([0, 256]);\n",
        "plt.xlabel('Token count');"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628541996528
        },
        "id": "YHFiORafztp1"
      },
      "source": [
        "MAX_LEN = 128"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628541999181
        },
        "id": "CMuzinY3ztp1"
      },
      "source": [
        "class SarcasmDataset(Dataset):\n",
        "\n",
        "  def __init__(self, tweets, targets, tokenizer, max_len):\n",
        "    self.tweets = tweets\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.tweets)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    tweet = str(self.tweets[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      tweet,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'tweet_text': tweet,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628542000583
        },
        "id": "14LqRdijztp2"
      },
      "source": [
        "RANDOM_SEED = 42\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7msK5fVwEsGd",
        "outputId": "5bfee3d7-456f-4257-e8da-c98eb0707958"
      },
      "source": [
        "print(device)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628542012951
        },
        "id": "Z9zIi1Ohztp2"
      },
      "source": [
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)\n",
        "df_val, df_test = train_test_split(df_test, test_size=0.3, random_state=RANDOM_SEED)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628542018012
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXtoaygLztp2",
        "outputId": "16e0312e-2b42-4b8e-963d-4e382d2fd074"
      },
      "source": [
        "df_train.shape, df_val.shape, df_test.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3052, 2), (534, 2), (230, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628542020610
        },
        "id": "3_O37NR9ztp3"
      },
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = SarcasmDataset(\n",
        "    tweets=df.Tweet.to_numpy(),\n",
        "    targets=df.Label.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628542023124
        },
        "id": "wgyQZJnWztp4"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628542025689
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btiS9udFztp4",
        "outputId": "2575450e-2c11-4bef-ec19-f7c430a61089"
      },
      "source": [
        "data = next(iter(train_data_loader))\n",
        "data.keys()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['tweet_text', 'input_ids', 'attention_mask', 'targets'])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628542028031
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMU_H5e7ztp5",
        "outputId": "635923b1-8563-40a9-8d37-1f0798a09eaa"
      },
      "source": [
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['targets'].shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 128])\n",
            "torch.Size([64, 128])\n",
            "torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628542051315
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTTaFbSBztp5",
        "outputId": "cae0dba2-4cd2-44e5-c438-1c79557d3f39"
      },
      "source": [
        "bert_model_1 = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "bert_model_1.resize_token_embeddings(len(tokenizer))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(30685, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628542052197
        },
        "id": "lNE8FFcDztp6"
      },
      "source": [
        "last_hidden_state, pooled_output = bert_model_1(\n",
        "  input_ids=encoding['input_ids'], \n",
        "  attention_mask=encoding['attention_mask'],\n",
        "  return_dict = False\n",
        ")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628542053097
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3NOI5vhztp6",
        "outputId": "4741025b-7eca-432f-9668-3d6a2924a0b1"
      },
      "source": [
        "last_hidden_state.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 32, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628542057526
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvGHv4fAztp7",
        "outputId": "f2c9d4a0-951b-4fc5-93e3-155534222079"
      },
      "source": [
        "bert_model_1.config.hidden_size"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628542059145
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q8PdTsdztp7",
        "outputId": "561b94dc-07ff-436d-c5a9-dd45389ea3f4"
      },
      "source": [
        "pooled_output.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628542062775
        },
        "id": "ezq-1nJzztp8"
      },
      "source": [
        "class SarcasmClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(SarcasmClassifier, self).__init__()\n",
        "    self.bert = bert_model_1\n",
        "    self.drop = nn.Dropout(p=0.4)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    returned = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    pooled_output = returned[\"pooler_output\"]\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628543480185
        },
        "id": "wnKxnnaqztp8"
      },
      "source": [
        "class_names=['1', '0']"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628542067080
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8taxt9wuztp9",
        "outputId": "511e46f5-a22d-4f6d-be94-5624302a4aa2"
      },
      "source": [
        "len(class_names)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgWI4j3aSwDl"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628542100563
        },
        "id": "VaWv_jkoztp9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "331a41cf-08f5-42de-ebb2-0435efcaf3d6"
      },
      "source": [
        "model = SarcasmClassifier(len(class_names))\n",
        "model.to(device)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SarcasmClassifier(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30685, 768)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (drop): Dropout(p=0.4, inplace=False)\n",
              "  (out): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE1CTyUNlFjp",
        "outputId": "023ae3a1-fb34-4afe-fb02-e5779b400150"
      },
      "source": [
        "model"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SarcasmClassifier(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30685, 768)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (drop): Dropout(p=0.4, inplace=False)\n",
              "  (out): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628542105754
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzSibseqztp9",
        "outputId": "0da1d22a-4679-44d5-f4a8-77f9543f3b67"
      },
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "\n",
        "print(input_ids.shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 128])\n",
            "torch.Size([64, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628542108826
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8g-06PX2ztp-",
        "outputId": "3146172d-d379-4f1d-c950-6865d05cca13"
      },
      "source": [
        "F.softmax(model(input_ids, attention_mask), dim=1)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4422, 0.5578],\n",
              "        [0.4610, 0.5390],\n",
              "        [0.6079, 0.3921],\n",
              "        [0.4842, 0.5158],\n",
              "        [0.6856, 0.3144],\n",
              "        [0.4569, 0.5431],\n",
              "        [0.6337, 0.3663],\n",
              "        [0.5822, 0.4178],\n",
              "        [0.4577, 0.5423],\n",
              "        [0.5039, 0.4961],\n",
              "        [0.4512, 0.5488],\n",
              "        [0.6756, 0.3244],\n",
              "        [0.3598, 0.6402],\n",
              "        [0.6425, 0.3575],\n",
              "        [0.3712, 0.6288],\n",
              "        [0.5996, 0.4004],\n",
              "        [0.5812, 0.4188],\n",
              "        [0.4177, 0.5823],\n",
              "        [0.6249, 0.3751],\n",
              "        [0.3779, 0.6221],\n",
              "        [0.7830, 0.2170],\n",
              "        [0.4157, 0.5843],\n",
              "        [0.6491, 0.3509],\n",
              "        [0.4270, 0.5730],\n",
              "        [0.6795, 0.3205],\n",
              "        [0.5273, 0.4727],\n",
              "        [0.5275, 0.4725],\n",
              "        [0.4497, 0.5503],\n",
              "        [0.5888, 0.4112],\n",
              "        [0.5701, 0.4299],\n",
              "        [0.6824, 0.3176],\n",
              "        [0.5425, 0.4575],\n",
              "        [0.3674, 0.6326],\n",
              "        [0.3975, 0.6025],\n",
              "        [0.3491, 0.6509],\n",
              "        [0.5114, 0.4886],\n",
              "        [0.5644, 0.4356],\n",
              "        [0.5844, 0.4156],\n",
              "        [0.3890, 0.6110],\n",
              "        [0.5202, 0.4798],\n",
              "        [0.7468, 0.2532],\n",
              "        [0.6379, 0.3621],\n",
              "        [0.6797, 0.3203],\n",
              "        [0.4928, 0.5072],\n",
              "        [0.4247, 0.5753],\n",
              "        [0.6990, 0.3010],\n",
              "        [0.5102, 0.4898],\n",
              "        [0.5830, 0.4170],\n",
              "        [0.4387, 0.5613],\n",
              "        [0.6822, 0.3178],\n",
              "        [0.3901, 0.6099],\n",
              "        [0.2862, 0.7138],\n",
              "        [0.5000, 0.5000],\n",
              "        [0.4568, 0.5432],\n",
              "        [0.5507, 0.4493],\n",
              "        [0.6809, 0.3191],\n",
              "        [0.3709, 0.6291],\n",
              "        [0.5710, 0.4290],\n",
              "        [0.7121, 0.2879],\n",
              "        [0.5091, 0.4909],\n",
              "        [0.3882, 0.6118],\n",
              "        [0.2600, 0.7400],\n",
              "        [0.6751, 0.3249],\n",
              "        [0.4843, 0.5157]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxaeD6kaLzBN"
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "# Separate the `weight` parameters from the `bias` parameters. \n",
        "# - For the `weight` parameters, this specifies a 'weight_decay_rate' of 0.01. \n",
        "# - For the `bias` parameters, the 'weight_decay_rate' is 0.0. \n",
        "optimizer_grouped_parameters = [\n",
        "    # Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'.\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.1},\n",
        "    \n",
        "    # Filter for parameters which *do* include those.\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628542133125
        },
        "id": "iL7bzkOxztp-"
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, eps= 1e-8)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628542136704
        },
        "id": "MS46adZbztp_"
      },
      "source": [
        "def train_epoch(\n",
        "  model, \n",
        "  data_loader, \n",
        "  loss_fn, \n",
        "  optimizer, \n",
        "  device, \n",
        "  scheduler, \n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628542138373
        },
        "id": "543L-EZfztp_"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9XPrBcpztqA",
        "outputId": "d892a2b7-f42e-4681-efb7-56e0403c16f4"
      },
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state_semeval_6.bin')\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.6683910600841045 accuracy 0.580602883355177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.6295582718319364 accuracy 0.6404494382022472\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.5328085925430059 accuracy 0.7250982961992136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.6435328655772738 accuracy 0.6779026217228464\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.34663534195472795 accuracy 0.8532110091743119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7984039121203952 accuracy 0.6685393258426966\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.17780285398475826 accuracy 0.9374180865006553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 1.1321216622988384 accuracy 0.6591760299625469\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.11581899334366123 accuracy 0.9616644823066841\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.9765186905860901 accuracy 0.6853932584269663\n",
            "\n",
            "CPU times: user 3min 3s, sys: 3.45 s, total: 3min 7s\n",
            "Wall time: 3min 8s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628543120514
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "zrAPw73lztqA",
        "outputId": "6190b806-e55f-4d56-fd33-7a881d95207b"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628543125557
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "-o0p2u2QztqC",
        "outputId": "ff41fda2-5f6c-498b-b6ad-dcd3eade2d54"
      },
      "source": [
        "plt.plot(history['train_loss'], label='train loss')\n",
        "plt.plot(history['val_loss'], label='validation loss')\n",
        "\n",
        "plt.title('Training Loss vs Validation Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfEAAAGDCAYAAAA72Cm3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3xW5f3/8dcnm4QkQNgJEBRE9gozbLEF696Cstx11NXW9tdh26+tVkXrqlULKoqKKIqKola2iOwpyAoQ9koIG5Lr98e5wYABAuTOyX3n/Xw88vC+zzn3uT/nviXvXNe5znXMOYeIiIiEngi/CxAREZEzoxAXEREJUQpxERGREKUQFxERCVEKcRERkRClEBcREQlRCnEp18zsMzMbWNLbSvGY2UQzuyXwuL+ZfVGcbc/gfeqa2W4zizzTWkXKIoW4hJzAL+MjPwVmtq/Q8/6nsy/nXF/n3Oslve3pMLMeZpZd0vstDWb2sJlNLmJ5VTM7aGbNirsv59xbzrmflVBdWWbWu9C+1zrnKjrn8kti/8e9lzOzBiW9X5HiUIhLyAn8Mq7onKsIrAUuKbTsrSPbmVmUf1WWG28Cnc2s/nHLrwcWOucW+VCTSLmhEJewcaRFa2a/NbNNwHAzq2xmn5jZVjPbGXicVug1hbtzB5nZVDN7MrDtajPre4bb1jezyWaWZ2ZfmdkLZvbmGRxT48D75pjZYjO7tNC6i8xsSeA91pvZQ4HlVQPHmWNmO8xsipn95N+6mf3bzJ48btlHZvZA4PFvA/vNM7NlZnbB8ftwzmUDXwM3HbdqAPDGqT7/4957kJlNLfT8QjNbama5ZvY8YIXWnWtmX5vZdjPbZmZvmVmlwLoRQF3g40DvzG/MLD3QYo4KbFPbzMYGPp8VZnZroX0/YmajzOyNwLEvNrOME3xFJ2RmyYF9bDWzNWb2hyPfg5k1MLNJgWPbZmbvBpabmT1tZlvMbJeZLTyd3gwpfxTiEm5qAlWAesBteP+PDw88rwvsA54/yes7AMuAqsA/gf+amZ3BtiOB74AU4BF+GnKnZGbRwMfAF0B14B7gLTNrFNjkv8DtzrlEoBlemAI8CGQD1YAawO+BouZXfhu47kjNZlYZ+BnwTuA97gbaBfb/cyDrBKW+Xvj4Aq9thfcZnO7nf2QfVYEPgD/gfb4rgczCmwD/AGoDjYE6eJ8zzrmbOLaH5p9FvMU7eJ9RbeBq4O9m1qvQ+ksD21QCxhan5iI8ByQD5wDd8f6wGRxY9ze877UykBbYFrzPvxtwXuC11wLbz+C9pZxQiEu4KQD+7Jw74Jzb55zb7px73zm31zmXBzyK9wv1RNY4514JnDt9HaiFF4TF3tbM6gLtgD855w4656biBcHp6ghUBB4L7Odr4BPghsD6Q0ATM0tyzu10zs0ptLwWUM85d8g5N8UVfZOEKXjh3jXw/GpgunNuA5APxAb2H+2cy3LOrTxBnWMCx9058HwA8JlzbusZfP5HXAQsds6Nds4dAp4BNh1Z6Zxb4Zz7MvA9bwWGFnO/mFkdvD8Ifuuc2++cmwe8Gqj7iKnOuXGB73YE0LI4+y70HpF4pxR+55zLc85lAU/x4x87h/D+sKkdqGFqoeWJwPmAOee+d85tPJ33lvJFIS7hZqtzbv+RJ2YWb2b/CXRn7gImA5XsxKOUCwfF3sDDiqe5bW1gR6FlAOtO8zgI7Gedc66g0LI1QGrg8VV4Ybcm0DXbKbD8CWAF8IWZrTKzh4vaeSDY3+HHPwr6AW8F1q0A7sNr3W4xs3fMrPYJ9rMXeA8YEGjV9wfegDP6/I859uNqPfrczGoEalof2O+beC324jjy/eQVWlb4c4VC3y2wF4iz0xtjURWIDuy3qPf4DV5vwneB7vohAIE/1J4HXsD73F82s6TTeF8pZxTiEm6Ob3E+CDQCOjjnkvC6KqHQ+dUg2AhUMbP4QsvqnMF+NgB1jjufXRdYD+Ccm+mcuwyvq/1DYFRgeZ5z7kHn3Dl43cIPFHU+O+Bt4Gozq4d3euD9IyuccyOdc13wWowOePwktb6O1/V7IV5L8uPA8jP9/DdS6DML/HFQ+DP8e6Cm5oH93njcPk92e8YNeN9PYqFlRz/XErKNH1vbP3kP59wm59ytzrnawO3AixYY4e6ce9Y51xZogtet/usSrEvCjEJcwl0i3nnYHDOrAvw52G/onFsDzAIeMbOYQAv5klO9zsziCv/gnVPfC/zGzKLNrEdgP+8E9tvfzJID3c278E4lYGYXBwZOGZCL1zVeUNR7Oufm4gXOq8B451xOYB+NzKyXmcUC+/E+wyL3ETAFyAFeBt5xzh0MLD/Tz/9ToKmZXRloAd+LN97hiERgN5BrZqn8NOg2452LLuqY1wHfAP8IfNYtgJvxWvNnKua47w68P6oeNbPEwB9JDxx5DzO7xn4c4LcT74+OAjNrZ2YdAuMh9uB99if73KWcU4hLuHsGqIAXVN8Cn5fS+/YHOuENSvo/4F3gwEm2T8ULu8I/dfBCuy9e/S8CA5xzSwOvuQnICnQn3xF4T4CGwFd4ITcdeNE5N+Ek7z0S6B347xGxwGOB992E19r/3Yl2EOjufgOv5flGoVVn9Pk757YB1wRq2B44pmmFNvkL0Abvj5RP8QbBFfYP4A/mjdB/qIi3uAFIx2uVj8EbR/FVcWo7gcUc+90NxhuIuAdYBUzF+3yHBbZvB8wws9144yV+5ZxbBSQBr+AF+xq8Y3/iLOqSMGdFj3cRkZIUuIRoqXMu6D0BIlJ+qCUuEgSBbtFzzSzCzPoAl+GdtxYRKTFBC3EzGxaYsKDIGZsCkxo8a95ECwvMrE2wahHxQU1gIl6X9rPAnYHzzyIiJSZo3elm1g3vF9gbzrmfzDhkZhfhnTO6CG9U7L+ccx2CUoyIiEgYClpL3Dk3Gdhxkk0uwwt455z7Fu/a0VrBqkdERCTc+HlOPJVjJ8DI5tjJFkREROQkQuIuT2Z2G9482CQkJLQ9//zzfa5IRMLa/l2wcxVEJ0DKufDT+8dIONu/C/I2wqG9EBkDiTUhvgrBnSPqxGbPnr3NOVetqHV+hvh6jp2BKY0TzJjknHsZbxIJMjIy3KxZs4JfnYiUT2u+gRFXQLXOMPATiNOsp+WSc7D8C5j4D9gwFyolQrdfQ8vrITK6VEsxszUnWufnn5djCcy1bGYdgVxN9C8ivto4H0ZeB8l14MYPFODlmRmc93O4dQL0G+W1xMfeDc+1hTkjIP+Q3xUCwb3E7G282aIamXeP55vN7A4zuyOwyTi8mYxW4M1Q9Mtg1SIickrblsOIKyEuGQZ8CAnFvZ+KhLUyHuYhN2ObutNFpMTlrINhfSD/AAz+HKo28LsiKat+0s1eL+jd7GY22zmXUeS6cAjxQ4cOkZ2dzf79+0/wKikr4uLiSEtLIzq6dM8piZzQ7q0wvA/s3gKDPoVaLfyuSEJBKYZ52If46tWrSUxMJCUlBe/GTVIWOefYvn07eXl51K9f3+9yRGB/Lrx2sdeVftMYqNfp1K8RKawUwvxkIR4W103s379fAR4CzIyUlBT1mEjZcGgfjLwetiyB60YowOXMFHXOfMKjUJBfKm8fEteJF4cCPDToe5IyIf8QjBoIa6fD1f+Fhhf6XZGEuiNh3vBnkJsN0XGnfk0JCIuWuN9ycnJ48cUXz+i1F110ETk5OcXe/pFHHuHJJ588o/cSEbwW0pg7YPl4uPhpaHaV3xVJODGDSnVOvV0JUYiXgJOF+OHDh0/62nHjxlGpUqVglCUix3MOxv0aFo2G3o9AxmC/KxI5KwrxEvDwww+zcuVKWrVqxa9//WsmTpxI165dufTSS2nSpAkAl19+OW3btqVp06a8/PLLR1+bnp7Otm3byMrKonHjxtx66600bdqUn/3sZ+zbt++k7ztv3jw6duxIixYtuOKKK9i5cycAzz77LE2aNKFFixZcf/31AEyaNIlWrVrRqlUrWrduTV5eXpA+DZEy7Ou/waz/QuavoMv9flcjctbC5pz4EX/5eDFLNuwq0X02qZ3Eny9pesL1jz32GIsWLWLevHkATJw4kTlz5rBo0aKjo7CHDRtGlSpV2LdvH+3ateOqq64iJSXlmP0sX76ct99+m1deeYVrr72W999/nxtvvPGE7ztgwACee+45unfvzp/+9Cf+8pe/8Mwzz/DYY4+xevVqYmNjj3bVP/nkk7zwwgtkZmaye/du4uJK53yNSJkx7VmY8hS0HQS9/+J3NSIlQi3xIGnfvv0xl1E9++yztGzZko4dO7Ju3TqWL1/+k9fUr1+fVq1aAdC2bVuysrJOuP/c3FxycnLo3r07AAMHDmTy5MkAtGjRgv79+/Pmm28SFeX9nZaZmckDDzzAs88+S05OztHlIuXC7Nfhyz9C0yvgF0O985YiYSDsfpOfrMVcmhISEo4+njhxIl999RXTp08nPj6eHj16FHmZVWxs7NHHkZGRp+xOP5FPP/2UyZMn8/HHH/Poo4+ycOFCHn74YX7xi18wbtw4MjMzGT9+PLobnJQLiz+ET+6DBr3hipchItLvikRKjFriJSAxMfGk55hzc3OpXLky8fHxLF26lG+//fas3zM5OZnKlSszZcoUAEaMGEH37t0pKChg3bp19OzZk8cff5zc3Fx2797NypUrad68Ob/97W9p164dS5cuPesaRMq8Ff+D92+BtPZw7QiIivG7IpESFXYtcT+kpKSQmZlJs2bN6Nu3L7/4xS+OWd+nTx9eeuklGjduTKNGjejYsWOJvO/rr7/OHXfcwd69eznnnHMYPnw4+fn53HjjjeTm5uKc495776VSpUr88Y9/ZMKECURERNC0aVP69u1bIjWIlFlrZ8C7N0K186HfuxAT73dFIiUuLKZd/f7772ncuLFPFcnp0vclQbdpEbx2EcRXhSGfQ8XqflckcsbCftpVEZGjtq+EEVdAdIJ3S1EFuIQxdaeLSPjYtQHeuBxcPgz4BCrV9bsikaBSiItIeNiz3QvwfTth0MdQrZHfFYkEnUJcRELfgTx462rYmQU3fQC1W/tdkUipUIiLSGg7tB/evgE2zofr34L0Ln5XJFJqFOIiErryD8PowZA1Fa58GRrp0kkpXzQ63ScVK1YEYMOGDVx99dVFbtOjRw+Ov5zueM888wx79+49+vx0b216IrrlqZR5BQXw0V2wbBxc9AS0uNbvikRKnULcZ7Vr12b06NFn/PrjQ1y3NpVywTn4/GFY8A70/AO0v9XvikR8oRAvAQ8//DAvvPDC0edHWrG7d+/mggsuoE2bNjRv3pyPPvroJ6/NysqiWbNmAOzbt4/rr7+exo0bc8UVVxwzd/qdd95JRkYGTZs25c9//jPg3VRlw4YN9OzZk549ewI/3toUYOjQoTRr1oxmzZrxzDPPHH0/3fJUQt7Ex+C7/0Cnu6HbQ35XI+Kb8Dsn/tnDsGlhye6zZnPo+9gJV1933XXcd9993HXXXQCMGjWK8ePHExcXx5gxY0hKSmLbtm107NiRSy+9FDvBHZT+/e9/Ex8fz/fff8+CBQto06bN0XWPPvooVapUIT8/nwsuuIAFCxZw7733MnToUCZMmEDVqlWP2dfs2bMZPnw4M2bMwDlHhw4d6N69O5UrV9YtTyW0fftvmPQYtLoRfvZ/uiOZlGtqiZeA1q1bs2XLFjZs2MD8+fOpXLkyderUwTnH73//e1q0aEHv3r1Zv349mzdvPuF+Jk+efDRMW7RoQYsWLY6uGzVqFG3atKF169YsXryYJUuWnLSmqVOncsUVV5CQkEDFihW58sorj94sRbc8lZA1b6TXjd74ErjkXwpwKffC7zfsSVrMwXTNNdcwevRoNm3axHXXXQfAW2+9xdatW5k9ezbR0dGkp6cXeQvSU1m9ejVPPvkkM2fOpHLlygwaNOiM9nOEbnkqIen7T+Cju+GcHnDVfyEy/H59iZwutcRLyHXXXcc777zD6NGjueaaawCvFVu9enWio6OZMGECa9asOek+unXrxsiRIwFYtGgRCxYsAGDXrl0kJCSQnJzM5s2b+eyzz46+5kS3Qe3atSsffvghe/fuZc+ePYwZM4auXbue9nHplqdSJqya6F1KltoGrnsLomJP+RKR8kB/ypaQpk2bkpeXR2pqKrVq1QKgf//+XHLJJTRv3pyMjIxTtkjvvPNOBg8eTOPGjWncuDFt27YFoGXLlrRu3Zrzzz+fOnXqkJmZefQ1t912G3369KF27dpMmDDh6PI2bdowaNAg2rdvD8Att9xC69atT9p1fiK65an4KnsWvN0PUhpAv1EQW9HvikTKDN2KVEqdvi8pts1LvFuKxiXDkPGQWNPvikRKnW5FKiKhZ8dq75aikbEw4CMFuEgR1J0uImVP3iYYcTnkH4DBn0HldL8rEimTFOIiUrbs3eG1wHdvhYFjobpOvYicSNh0p4fauf3ySt+TnNSB3TDyWti+Am4YCWlFngYUkYCwCPG4uDi2b9+ugCjjnHNs375ds7hJ0Q4fgHdvhPWz4erh3vXgInJSYdGdnpaWRnZ2Nlu3bvW7FDmFuLg40tLS/C5Dypr8w/D+zbBqAlz+b2h8sd8ViYSEsAjx6Oho6tev73cZInImnIOPfwXffwx9HoNW/fyuSCRkhEV3uoiEKOfgiz/AvDeh+8PQ8U6/KxIJKQpxEfHPlCdh+vPQ/nbo8bDf1YiEHIW4iPjju1fg6/+DFtd73ei6I5nIaVOIi0jpWzAKxj0EjS6Cy56HCP0qEjkT+pcjIqVr2Wcw5g5I7+pdShYZ7XdFIiFLIS4ipSdrKrw3CGq1hBvehmjNGSByNhTiIlI61s+Bkdd786D3Hw2xiX5XJBLyFOIiEnxbl8GbV0F8ZbhpDCSk+F2RSFhQiItIcOWs9W5oEhEFN30ISbX9rkgkbITFjG0iUkbt3gJvXA4Hd8OgcZByrt8ViYQVhbiIBMe+HBhxJeRt9FrgNZv5XZFI2FGIi0jJO7gXRl4HW5dCv3egbge/KxIJSwpxESlZhw/CqJsg+zvvOvAGvf2uSCRsKcRFpOQU5MOY22DFV3Dpc9D0cr8rEglrGp0uIiXDOfjkflg8Bi78G7QZ4HdFImFPIS4iJeOrR2DO69D1Qci81+9qRMoFhbiInL2pT8O0ZyDjZuj1R7+rESk3FOIicnZmDfda4c2uhoue1C1FRUqRQlxEztyi973z4A1/Dle8pFuKipQy/YsTkTOz/Ev44Dao2wmueU23FBXxgUJcRE7fmunw7k1Qo6k3mUtMvN8ViZRLQQ1xM+tjZsvMbIWZPVzE+rpmNsHM5prZAjO7KJj1iEgJ2LjAm40tOQ1u/ADikv2uSKTcClqIm1kk8ALQF2gC3GBmTY7b7A/AKOdca+B64MVg1SMiJWDbCu+OZLGJgVuKVvW7IpFyLZgt8fbACufcKufcQeAd4LLjtnFAUuBxMrAhiPWIyNnIzYYRgRnYBnwEler4W4+IBHXa1VRgXaHn2cDxd0F4BPjCzO4BEgBNsixSFu3a4LXA9+fCoE+gagO/KxIR/B/YdgPwmnMuDbgIGGFmP6nJzG4zs1lmNmvr1q2lXqRIubVrI3z2W3i2NeSsg37vQq2WflclIgHBbImvBwr3t6UFlhV2M9AHwDk33czigKrAlsIbOedeBl4GyMjIcMEqWEQCdm2Aqc/A7Neg4DC0ugG6PgRV6vtdmYgUEswQnwk0NLP6eOF9PdDvuG3WAhcAr5lZYyAOUFNbxC+7NnhTqM5+HVw+tLzBmwtd4S1SJgUtxJ1zh83sbmA8EAkMc84tNrO/ArOcc2OBB4FXzOx+vEFug5xzammLlLbc9V54z3kdXAG06ueFd+V0vysTkZMI6v3EnXPjgHHHLftTocdLgMxg1iAiJ5GbHQjvNwLh3T8Q3vX8rkxEiiGoIS4iZVRuNkwZCnNHePcBb90fujyg8BYJMQpxkfIkZx1MHQpzRnjPW98IXR+ASnX9rUtEzohCXKQ8yFkbaHm/6T1vc5PX8taELSIhTSEuEs5y1sKUp2DuW959vtsMgC73K7xFwoRCXCQc7Vzjhfe8t8AioO1AL7yT0/yuTERKkEJcJJzszILJT8L8t73wzhgCmfdBcqrflYlIECjERcLBjtUw5UmY/w5YJGTcDF3ug6TaflcmIkGkEBcJZTtWweSnvJZ3RBS0u8VreSfV8rsyESkFCnGRULR9pXfOe/47EBkN7W+DzF8pvEXKGYW4SCjZvtI7573gXS+8O9zuhXdiTb8rExEfKMRFQsH2lTD5CVgwKhDed0DmvQpvkXJOIS5Slm1b4YX3wlEQGQsd74TO90JiDb8rE5EyQCEuUhZtWx4I7/cC4f1Lr9u8YnW/KxORMkQhLlKWbP3BC+9FoyEqDjrd5bW8Fd4iUgSFuEhZsPUHmPxPWDgaoitAp7sD4V3N78pEpAxTiIv4aesymPRPWPQ+RMd7g9U63wsJVf2uTERCgEJcxA9blnot70UfBML7V9D5HoW3iJwWhbhIadryPUx6HBZ/CDEJ3tSone6BhBS/KxOREKQQFykNm5d44b3ko0B43++d91Z4i8hZUIiLBNPmxYXCOxG6PuiNOI+v4ndlIhIGFOIiwbBpkRfe34/1wrvbr71rvRXeIlKCFOIiJWnTwkB4fwyxSdDtN94sawpvEQkChbhISdi4wAvvpZ944d39t154V6jsd2UiEsYU4iJnY+N87zrvpZ9AbDJ0fxg63qHwFpFSoRAXORMb58PEx2HZp1549/idd2exCpX8rkxEyhGFuMjp2DDP6zZfNg7ikqHH7717eiu8RcQHCnGR4tgw12t5//CZF949/58X3nHJflcmIuWYQlzkZNbP8VreP3wOcZWg5x+gw20KbxEpExTiIkVZP9treS8f7w1S6/UHaH87xCX5XZmIyFEKcZHCsmfBxMdgxZeB8P4jtL9N4S0iZZJCXARg3UyY9Bis+AoqVIEL/uSFd2yi35WJiJyQQlzKt3XfeS3vlf8LhPefof2tCm8RCQkKcSmf1n0HE/8BK7+G+BTo/Qi0uxViK/pdmYhIsSnEpXxZO8ML71UTAuH9F2h3i8JbREKSQlzKhzXTvXPeqyZCfFW48G/Q7mbv3t4iIiFKIS7hbc033jnv1ZMgoRr87P8gY4jCW0TCQrkO8cP5BRzKd1SIifS7FClpWdO8lvfqyYHwfjQQ3vF+VyYiUmLKdYh/sWQzvx+zkBva12VAp3rUSq7gd0lytrKmei3vrCmQUB1+/ndoO1jhLSJhqVyHeN0q8XQ6J4X/TFrJy5NX0bdZTQZn1qdN3UqYmd/lyelYPcWbHjVrClSsAT//B7QdpPAWkbBmzjm/azgtGRkZbtasWSW6z3U79jLi2zW8/d1a8vYfpmWdSgzJTKdvs1rEREWU6HtJCVs9xWt5r5nqhXeX+73wjlavioiEBzOb7ZzLKHKdQvxHew4c5oM52QyflsWqbXuokRTLgE7p3NC+LlUSYoLynnIGnPNa3BMfgzXToGLNQHgPVHiLSNhRiJ+mggLHpOVbGTZ1NVOWbyM2KoIrWqcyOLM+jWpqJi/fOOeNMp/4OKz9BhJreeHdZiBEx/ldnYhIUCjEz8LyzXkM/yaLD+Zks/9QAZkNUhjcuT69zq9ORITOm5cK57zruyc9DmunQ2LtQHgPUHiLSNhTiJeAnXsO8vbMtYyYvoaNuftJT4lnUOd0rs6oQ8XYcj0+MHic82ZWm/g4rPvWC++uD0DrmxTeIlJuKMRL0KH8Aj5ftInh01YzZ20OibFRXNuuDoM6p1OnikZClwjnvDnNJz0O62ZAUuqPLe+oWL+rExEpVQrxIJm3Lofh01bz6YKNFDhH78Y1GNKlPh3qVymfl6gdPggHd8OhvXBwz48/h/Z6yw8Glh86sm5v0dvv2wE7syApDbre77W8Fd4iUk4pxINsU+5+RnybxcgZa9m59xCNayUxJDOdS1rWJi66jM0G5xwc2ndscJ5uyJ5o+4LDxa/DIr2pT2MSIDr+x8dHnp/TA1rfqPAWkXJPIV5K9h/K58O56xk+LYtlm/NISYihf8d63NixLtUTT/McbkFBIBxPM0h/sn0Rr+c0vvPImEDIVvQmTolJgOgjgVvU84pFhHIR66NioTz2VoiInCaFeDDlHyoUql5IuoO7WbJmE5MXZ7Fy/RYSIw7QtmYs7VNjqR6XX7xQPrzv9OqIqnAaoXqyUD5u+8jo4HxuIiJSLCcL8fI9rDp3PWxaWMyW7glavvkHf7JbA5oGfjiSgVuhYIuxLyKOiJgEYuITsSNBGZsIiTXPLGSPbB9RxrrtRUQk6Mp3iK/8H4y956fLLTIQosd1GcdXhUona92eOIR3uRhGzd3G69+uYd2OfaRaBQa2rsd17eqSXEGtXREROX3luzt991bIXVtEF3JM0M7X5hc4vvp+M8OmrmbG6h3Ex0RyVZs0BmWmc261ikF5TxERCV06J15GLd6Qy/BpWYydt4GD+QX0aFSNIZn16dqwavm8RE1ERH5CIV7Gbc07wMgZaxnx7Rq27T5Aw+oVGZSZzpWt06gQo3PdIiLlmUI8RBw4nM+nCzYybNpqFq3fRaX4aG5oX5cBnepRK1l35xIRKY8U4iHGOcesNTsZNnU14xdvwszo26wmgzPr06ZuJXW1i4iUI7rELMSYGe3Sq9AuvQrrduxlxLdrePu7tXyyYCMt61RiSGY6fZvVIiYqwu9SRUTER0FtiZtZH+BfQCTwqnPusSK2uRZ4BG8asfnOuX4n22d5aIkXZc+Bw3wwJ5vh07JYtW0PNZJiGdApnRva16VKQozf5YmISJD40p1uZpHAD8CFQDYwE7jBObek0DYNgVFAL+fcTjOr7pzbcrL9ltcQP6KgwDFp+VaGTV3NlOXbiI2K4IrWqQzOrE+jmol+lyciIiXMr+709sAK59yqQBHvAJcBSwptcyvwgnNuJ8CpAlwgIsLo2ag6PRtVZ/nmPIZ/k8UHc7J5Z+Y6MhukMLhzfXqdX52ICJ03FxEJd8E8qZoKrCv0PDuwrLDzgPPMbJqZfRvofv8JM7vNzGaZ2aytW7cGqdzQ07BGIn+/ojnTH76A3/RpxKqte7jljYPM1AgAACAASURBVFn0emoir01bze4Dp3FXMRERCTnB7E6/GujjnLsl8PwmoINz7u5C23wCHAKuBdKAyUBz51zOifZb3rvTT+ZQfgHjF29i2NTVzFmbQ2JsFNe2q8PATunUTYn3uzwRETkDfnWnrwfqFHqeFlhWWDYwwzl3CFhtZj8ADfHOn8tpio6M4OIWtbm4RW3mrcth+LTVvP5NFsOmrebCxjUY0qU+HepX0SVqIiJhIpgt8Si8gW0X4IX3TKCfc25xoW364A12G2hmVYG5QCvn3PYT7Vct8dOzKXc/I77NYuSMtezce4jGtZIYkpnOJS1rExet2eBERMo63yZ7MbOLgGfwLjEb5px71Mz+Csxyzo01r0n4FNAHyAcedc69c7J9KsTPzP5D+Xw4dz3Dp2WxbHMeKQkx9O9Yjxs71qV6Ypzf5YmIyAloxjY5yjnHNyu3M3zaav63dAtREcYlLWozOLM+zdOS/S5PRESOoxnb5CgzI7NBVTIbVGX1tj28/k0W781axwdz19MuvTJDMutzYZMaREVqNjgRkbJOLXFh1/5DjJq5jtenZ7Fuxz5SK1VgYOd6XNeuLskVov0uT0SkXFN3uhRLfoHjq+83M2zqamas3kF8TCRXtUljUGY651ar6Hd5IiLlkkJcTtviDbkMn5bF2HkbOJhfQI9G1RiSWZ+uDavqEjURkVKkEJcztjXvACNnrGXEt2vYtvsADatXZFBmOle2TqNCjC5RExEJNoW4nLUDh/P5dMFGhk1bzaL1u6gUH8317eoyoFM9aleq4Hd5IiJhSyEuJcY5x6w1Oxk2dTXjF2/CzOjTrCZDMuvTpm4ldbWLiJQwXWImJcbMaJdehXbpVVi3Yy8jvl3D29+t5dMFG2lZpxJDMtPp26wWMVG6RE1EJNjUEpeztufAYT6Yk83waVms2raHGkmx3NSxHv061KNKQozf5YmIhDR1p0upKChwTFq+lWFTVzNl+TZioyK4onUqgzPr06hmot/liYiEJHWnS6mIiDB6NqpOz0bVWb45j+HfZPHBnGzembmOzAYpDO5cn17nVyciQufNRURKglriElQ79xzk7ZlrGTF9DRtz95OeEs/Azulck1GHirH6G1JE5FTUnS6+O5RfwPjFmxg2dTVz1uaQGBvFte3qMLBTOnVT4v0uT0SkzFKIS5kyb10Ow6et5tMFG8l3jgsb12BIl/p0qF9Fl6iJiBxHIS5l0qbc/Yz4NouRM9ayc+8hGtdKYkhmOpe0rE1ctGaDExEBhbiUcfsP5fPh3PUMn5bFss15pCTEcEf3cxmUmU60bokqIuWcQlxCgnOOb1Zu56VJK5myfBsNqlfkr5c2pXODqn6XJiLim5OFuJo5UmaYGZkNqjLi5g68OiCDA4fz6ffqDO4eOYdNufv9Lk9EpMwpVoibWYKZRQQen2dml5pZdHBLk/Ksd5MafHl/d+7r3ZAvl2ym11MTeWnSSg4eLvC7NBGRMqO4LfHJQJyZpQJfADcBrwWrKBGAuOhI7ut9Hl/e353O56bw2GdL6fuvyUxbsc3v0kREyoTihrg55/YCVwIvOueuAZoGryyRH9VNiefVge0YNiiDQ/mO/q/O4K635rAxd5/fpYmI+KrYIW5mnYD+wKeBZboGSEpVr/Nr8MX93bi/93l89f1mLnhqEv+eqC52ESm/ihvi9wG/A8Y45xab2TnAhOCVJVK0uOhIftW7IV890J3MBlV5/POl9PnXZKYs3+p3aSIipe60LzELDHCr6JzbFZySTk6XmElhE5Zu4ZGPF7Nm+176NqvJHy5uQmqlCn6XJSJSYs76EjMzG2lmSWaWACwClpjZr0uySJEz0fP86oy/rxsPXngeE5ZtofdTk3hhwgoOHM73uzQRkaArbnd6k0DL+3LgM6A+3gh1Ed/FRUdyzwVeF3u386ryxPhl9HlmCpN+UBe7iIS34oZ4dOC68MuBsc65Q0BoTfUmYS+tcjz/uSmD1wa3wznHwGHfcfuIWWTv3Ot3aSIiQVHcEP8PkAUkAJPNrB7gyzlxkVPp0ag64+/vxq9/3ohJP2yl99BJPP/1cnWxi0jYOeO5080syjl3uITrOSUNbJPTsT5nH//3yRI+W7SJ9JR4/nxpU3o2qu53WSIixVYSA9uSzWyomc0K/DyF1yoXKdNSK1Xg3ze25Y0h7YkwY/Dwmdz6xizW7VAXu4iEvuJ2pw8D8oBrAz+7gOHBKkqkpHU7rxqf3deV3/RpxNTl2+g9dBLP/m85+w+pi11EQlexutPNbJ5zrtWplpUGdafL2dqQs49HP/2eTxdupF5KPH++pAm9zq/hd1kiIkUqiVuR7jOzLoV2mAlo4moJSbUrVeCF/m148+YOREYYQ16bxS2vq4tdREJPcVviLYE3gOTAop3AQOfcgiDWViS1xKUkHTxcwLBpq3n2f8vJL3Dc2eNc7uh+LnHRujWAiJQNZ90Sd87Nd861BFoALZxzrYFeJVijiC9ioiK4o/u5/O/B7vRuUoNnvlrOhU9P4qslm/0uTUTklIrbnQ6Ac25XoTnTHwhCPSK+qJVcgRf6teGtWzoQGxXJLW/M4ubXZrJ2u7rYRaTsOq0QP46VWBUiZURmg6qMu7crv7/ofKav2k7vpycx9MsfNIpdRMqkswlxTbsqYSkmKoLbup3L1w/24OdNa/Ls/5bTe+gkvli8iTOdHElEJBhOGuJmlmdmu4r4yQNql1KNIr6omRzHcze0ZuStHagQHcltI2Yz5LWZZG3b43dpIiLAWUy76heNThc/HMov4PVvsnj6yx84lO+4vfs5/LJHAyrEaBS7iARXSVwnLlKuRUdGcEvXc/j6oR70bV6T575eQe+hkxivLnYR8ZFCXOQ01EiK41/Xt+ad2zpSMTaK20fMZtDwmaxWF7uI+EAhLnIGOp6Twif3duGPFzdh9pqd/PzpyTwxfil7D5b6jf1EpBxTiIucoejICG7uUp+vH+zOL1rU4oUJK7lw6GQ+X7RRXewiUioU4iJnqXpSHE9f14pRt3ciMS6KO96cw4Bh37Fq626/SxORMKcQFykh7etX4ZN7uvCni5swb20OP39mMv/8XF3sIhI8CnGREhQVGcGQLvX530PduaRlbV6cuJLeT01i3EJ1sYtIyVOIiwRB9cQ4hl7bivfu6ERShWh++dYcbvrvd6zYoi52ESk5CnGRIGqX7nWxP3JJE+Zn59D3X5P5x2ffs+eAuthF5OwpxEWCLCoygkGZ9fn6wR5c1iqV/0xaxQVPTeKTBRvUxS4iZ0UhLlJKqiXG8uQ1LXn/zk5USYjh7pFzufG/M1ixJc/v0kQkRCnERUpZ23pV+PieLvz1sqYszM6lzzNT+Pu479mtLnYROU0KcREfREYYAzql8/VDPbiyTSovT17FBU9NZOx8dbGLSPEpxEV8VLViLP+8uiXv39mZqhVjufftufR7ZQY/bFYXu4icmkJcpAxoW68yY+/uwt8ub8aSjbu46F9TePTTJepiF5GTUoiLlBGREcZNHevx9YPdubptGq9MWU2vJyfy0bz16mIXkSIFNcTNrI+ZLTOzFWb28Em2u8rMnJkVedNzkfIkpWIsj13VgjG/7EyNpDh+9c48rn/5W5ZtUhe7iBwraCFuZpHAC0BfoAlwg5k1KWK7ROBXwIxg1SISilrXrcyHd2Xy6BXNWLY5j4uencLfPllC3v5DfpcmImVEMFvi7YEVzrlVzrmDwDvAZUVs9zfgcWB/EGsRCUmREUb/DvX4+sEeXJuRxrBpq+n11CTGzM1WF7uIBDXEU4F1hZ5nB5YdZWZtgDrOuU9PtiMzu83MZpnZrK1bt5Z8pSJlXJWEGP5xZQs+/GUmtZPjuP/d+Vz3n2/5fuMuv0sTER/5NrDNzCKAocCDp9rWOfeycy7DOZdRrVq14BcnUka1rFOJMb/M5B9XNueHLXlc/NxU/vLxYnapi12kXApmiK8H6hR6nhZYdkQi0AyYaGZZQEdgrAa3iZxcRIRxQ/u6THiwB9e1q8Nr32TR68lJvD9bXewi5U0wQ3wm0NDM6ptZDHA9MPbISudcrnOuqnMu3TmXDnwLXOqcmxXEmkTCRuWEGP5+RXM+uiuT1MoVePC9+Vzz0nSWbFAXu0h5EbQQd84dBu4GxgPfA6Occ4vN7K9mdmmw3lekvGmRVokxd3bm8auas2rbHi5+bgqPjF1M7j51sYuEOwu17reMjAw3a5Ya6yJFydl7kCe/WMZbM9aSkhDDw30bc2XrVCIizO/SROQMmdls51yRp5o1Y5tIGKkUH8P/Xd6cj+/uQp0q8Tz03nyu+c90Fm/I9bs0EQkChbhIGGqWmsz7d3Tmn1e3YPW2PVzy3FT+9NEicveqi10knCjERcJURIRxbUYdJjzYgxs71uPNb9fQ66mJjJq1joKC0DqNJiJFU4iLhLnk+Gj+elkzxt7dhXop8fxm9AKueukbFq1XF7tIqFOIi5QTzVKTGX1HZ564ugVrt+/lkuen8ocPF5Kz96DfpYnIGVKIi5QjERHGNRl1+PqhHgzslM7IGWvp9dQk3p25Vl3sIiFIIS5SDiVXiOaRS5vy8T1dOKdqAr99fyFX/vsbFmari10klCjERcqxprWTee+OTjx1TUuyd+7j0hem8vsxC9m5R13sIqFAIS5SzpkZV7VN4+uHujOoczrvzlxHr6cm8vZ36mIXKesU4iICQFJcNH++pCmf3NOFBtUr8rsPFnLFi9OYvy7H79JE5AQU4iJyjMa1khh1eyeevq4lG3L3c/mL0/jdB+piFymLFOIi8hNmxhWt0/j6we4MyazPqFnr6PHkRF6evJL9h/L9Lk9EAhTiInJCiXHR/PHiJoy7tyst61Ti7+OWcsFTk/hgTrbOl4uUAQpxETmlRjUTeWNIe966pQOVE6J5YNR8fvHcVCb9sJVQuxOiSDhRiItIsWU2qMrYu7rwr+tbsfvAIQYO+47+r87Q9eUiPlGIi8hpiYgwLmuVylcPdOdPFzfh+427uOT5qdzz9lzWbt/rd3ki5YqFWldYRkaGmzVrlt9liEjArv2H+M+klfx36mryCxz9O9Tjnl4NSKkY63dpImHBzGY75zKKXKcQF5GSsCl3P8989QOjZq0jPiaKO7qfw5Au9YmPifK7NJGQphAXkVKzfHMej3++jK++30z1xFjuv/A8rmmbRlSkzt6JnImThbj+VYlIiWpYI5FXB2bw3h2dSKtcgd99sJA+/5rCF4s3aSS7SAlTiItIULRLr8L7d3bmpRvbUlDguG3EbK55aTqz1+zwuzSRsKEQF5GgMTP6NKvJ+Pu78X+XNyNr+16u+vd0bh8xi5Vbd/tdnkjI0zlxESk1ew4c5r9TV/OfSSvZf7iA69rV4b4LGlI9Kc7v0kTKLA1sE5EyZdvuAzz3v+W8NWMt0ZER3NK1Prd1O4fEuGi/SxMpcxTiIlImZW3bwxNfLOPTBRtJSYjhnl4N6NehHjFROtMncoRGp4tImZReNYEX+rXho7syaVijIo98vIQLn57Ex/M3aCS7SDEoxEXEdy3rVOLtWzsyfFA74qIiueftuVz2wjS+WbnN79JEyjSFuIiUCWZGz/OrM+5XXXni6hZszTtAv1dmMGj4dyzdtMvv8kTKJJ0TF5Eyaf+hfF77JosXJ6wg78BhrmydxgM/O4/UShX8Lk2kVGlgm4iErJy9B3lhwgpe/2YNGAzunM4vezQgOV4j2aV8UIiLSMjL3rmXoV/8wJh560mKi+aunucyoFM6cdGRfpcmElQanS4iIS+tcjxDr2vFp/d0pVWdSvx93FIueGoS78/OJr8gtBojIiVFIS4iIaVJ7SReH9Kekbd0oEpCDA++N59fPDuFCcu26LI0KXcU4iISkjo3qMpHd2Xy7A2t2XPwMIOHz6TfKzNYkJ3jd2kipUYhLiIhKyLCuLRlbf73QA/+fEkTlm3O49Lnp3HP23NZs32P3+WJBJ0GtolI2Ni1/xAvT1rFq1NXkV/g6N+hHvf0akBKxVi/SxM5YxqdLiLlyuZd+3nmqx94d+Y64mOiuL3bOdzctT7xMVF+lyZy2hTiIlIurdiSx+OfL+PLJZupnhjLfb3P49qMNKIidSZRQocuMRORcqlB9UReGZDBe3d0Iq1yBX4/ZiE/f2Yy4xdv0kh2CQsKcREJe+3Sq/D+nZ156ca2OOD2EbO5+qXpzF6zw+/SRM6KQlxEygUzo0+zmnxxXzcevaIZa3fs5ap/T+e2N2axYstuv8sTOSM6Jy4i5dLeg4d5dcpq/jNpJfsPF3BtRh3u792Q6klxfpcmcgwNbBMROYFtuw/w/NcrePPbNURHRnBL1/rc1u0cEuN0gxUpGxTiIiKnsGb7Hp4Yv4xPFmykSkIM9/ZqQL8O9YiJ0llH8ZdGp4uInEK9lASe79eGj+7KpFGNRB75eAm9h07i4/kbKNANVqSMUoiLiBTSsk4lRt7ageGD2xEfE8k9b8/l8hen8c3KbX6XJvITCnERkeOYGT0bVefTe7vy5DUt2ZZ3gH6vzGDgsO/4fuMuv8sTOUohLiJyApERxtVt0/j6oR78ru/5zF27k4uencIDo+axPmef3+WJaGCbiEhx5ew9yIsTV/LaN1kADOqczl09GpAcr5HsEjwanS4iUoLW5+zjqS+WMWbuehJjo7irZwMGdk4nLjrS79IkDGl0uohICUqtVIGh17bi03u60rpuZf7x2VJ6PTmR0bOzyddIdilFCnERkTPUpHYSrw9pz8hbOpBSMZaH3pvPL56dwoRlW3SDFSkVCnERkbPUuUFVPrork+duaM3eg/kMHj6Tfq/MYEF2jt+lSZhTiIuIlICICOOSlrX56oHuPHJJE5ZtzuPS56dx98g5rNm+x+/yJExpYJuISBDk7T/Ey5NX8eqU1RwuKKB/h3rc06sBKRVj/S5NQoxGp4uI+GTLrv08/dVyRs1aR4XoSG7rdg63dK1PfEyU36VJiPBtdLqZ9TGzZWa2wsweLmL9A2a2xMwWmNn/zKxeMOsRESlt1ZPi+MeVzRl/Xzc6n5vC0C9/oPsTE3lrxhoO5xf4XZ6EuKCFuJlFAi8AfYEmwA1m1uS4zeYCGc65FsBo4J/BqkdExE8Nqlfk5QEZjL6jE3WrxPP/xiziZ89M5vNFmzSSXc5YMFvi7YEVzrlVzrmDwDvAZYU3cM5NcM7tDTz9FkgLYj0iIr7LSK/C6Ds68Z+b2gJwx5uzufql6czK2uFzZRKKghniqcC6Qs+zA8tO5GbgsyDWIyJSJpgZP29aky/u68bfr2jO2h17ufql6dz6xixWbMnzuzwJIWXiEjMzuxHIAJ44wfrbzGyWmc3aunVr6RYnIhIkUZER9OtQl0m/7sGDF57H9JXb+dnTk/ndBwvYvGu/3+VJCAhmiK8H6hR6nhZYdgwz6w38P+BS59yBonbknHvZOZfhnMuoVq1aUIoVEfFLfEwU91zQkEm/7sGATumMnp1N9ycm8OT4ZeTtP+R3eVKGBe0SMzOLAn4ALsAL75lAP+fc4kLbtMYb0NbHObe8OPvVJWYiEu7WbN/DE+OX8cmCjVRJiOGeXg3o36EeMVFlovNUSpkvl5g55w4DdwPjge+BUc65xWb2VzO7NLDZE0BF4D0zm2dmY4NVj4hIqKiXksDz/dow9u5MGtVI5C8fL6H30EmMnb+BAt1gRQrRZC8iImWYc46JP2zl8c+WsnRTHs1Tk3m47/lkNqjqd2lSSnQrUhGREGVm9GxUnU/v7cpT17Rk++4D9H91BgOGfceSDbv8Lk98ppa4iEgI2X8onzemZ/H81yvIO3CYxjWTaJ6aTLO0ZJqnJnN+zUTioiP9LlNKkOZOFxEJM7l7D/H69CxmZu1g4fpccvZ6o9ijIoyGNRJpnhoI99RkGtdKUrCHMIW4iEgYc86RvXMfi9bnsjDws2h9LjsDwR4ZYTSsXpHmqck0T/OCvYmCPWQoxEVEyhnnHOtzCgf7Lhatz2XHnoPAj8HeLDX5aIu9Sa0kKsQo2Muak4W47oUnIhKGzIy0yvGkVY6nT7NagBfsG3L3szA792i4T1i6hdGzswEv2BtUOxLsSTRPS6ZJrWQFexmmEBcRKSfMjNRKFUitVIE+zWoCXrBvzN1/tAt+4fpcJv2whffneMEeYd4d2I602JunJtOkdpLuh15G6FsQESnHzIzalSpQu1IFft70x2DftOvYFvvkH7bxwRxv5uwIg3OrVaR5moLdb/rERUTkGGZGreQK1EquwM8KBfvmXQeOGTg3ZXkRwR44v+51xSeREKuYCSZ9uiIickpmRs3kOGomx3FhkxpHl28OtNiPBPvUFdv4YO76wGuOC/bUZJrWVrCXJH2SIiJyxmokxVGjSRy9CwX7ll37j2mxf7NyG2MKBfs5VROODfbUZCoq2M+IPjURESlR1ZPiuCApjgsanzjYp6/azofzNgBesNcPBPuRcG9aO4nEuGi/DiFkKMRFRCToigz2vP3ewLnsXSxcn8uMVTv4KBDs4LXYC1/H3jQ1iSQF+zEU4iIi4ovqiXH0Oj+OXuf/GOxb8w4cM/PczKwdjJ3/Y7DXPxrsSTQLhHt5DnaFuIiIlBnVEmPpeX51ep5f/eiybbu9UfGLAgPoZmft4ONCwZ6eEn/MdexNU5NJrlA+gl0hLiIiZVrVirH0bFSdno1+DPbtR4I90GKfuzaHTxZsPLq+3nHB3qx2Msnx4RfsCnEREQk5KRVj6dGoOj0KBfuOPQd/DPbsXOatzeHTQsFet0r8MaPim6UmUSk+xo/yS4xCXEREwkKVhBi6n1eN7udVO7psx56DR1vri9bnMj87h08X/hjsdapUOCbYm6cmh1SwK8RFRCRsVUmIodt51ehWKNh37jnIog25x3THj1u46ej6tMo/DfbKCWUz2BXiIiJSrlROiKFrw2p0bfhjsOfsPcii9buOCfbPFv0Y7KmVKhxzP/bmqclUKQPBrhAXEZFyr1J8DF0aVqVLw6pHl+XuPXS0xX4k3D9ffGywN0tNOqbVnlIxtlTrVoiLiIgUITk+mswGVclsUCjY9x1i8fpjg3384s1H19dOjqN5WjIv9m9LZIQFvUaFuIiISDElV4imc4OqdD4+2Dcc6YbfRc7eg6US4KAQFxEROSvJFaLpfG5VOp9b9dQbl7CIUn9HERERKREKcRERkRClEBcREQlRCnEREZEQpRAXEREJUQpxERGREKUQFxERCVEKcRERkRClEBcREQlRCnEREZEQpRAXEREJUQpxERGREKUQFxERCVEKcRERkRClEBcREQlRCnEREZEQpRAXEREJUQpxERGREKUQFxERCVEKcRERkRClEBcREQlRCnEREZEQpRAXEREJUQpxERGREKUQFxERCVEKcRERkRClEBcREQlRCnEREZEQpRAXEREJUQpxERGREKUQFxERCVEKcRERkRAV1BA3sz5mtszMVpjZw0WsjzWzdwPrZ5hZejDrERERCSdBC3EziwReAPoCTYAbzKzJcZvdDOx0zjUAngYeD1Y9IiIi4SaYLfH2wArn3Crn3EHgHeCy47a5DHg98Hg0cIGZWRBrEhERCRvBDPFUYF2h59mBZUVu45w7DOQCKUGsSUREJGxE+V1AcZjZbcBtgae7zWxZCe6+KrCtBPfnJx1L2RQuxxIuxwE6lrIoXI4DSv5Y6p1oRTBDfD1Qp9DztMCyorbJNrMoIBnYfvyOnHMvAy8Ho0gzm+WcywjGvkubjqVsCpdjCZfjAB1LWRQuxwGleyzB7E6fCTQ0s/pmFgNcD4w9bpuxwMDA46uBr51zLog1iYiIhI2gtcSdc4fN7G5gPBAJDHPOLTazvwKznHNjgf8CI8xsBbADL+hFRESkGIJ6Ttw5Nw4Yd9yyPxV6vB+4Jpg1FENQuul9omMpm8LlWMLlOEDHUhaFy3FAKR6LqfdaREQkNGnaVRERkRBVbkI8nKaALcaxDDKzrWY2L/Bzix91noqZDTOzLWa26ATrzcyeDRznAjNrU9o1FlcxjqWHmeUW+k7+VNR2fjOzOmY2wcyWmNliM/tVEduExPdSzGMJle8lzsy+M7P5gWP5SxHblPnfYcU8jpD4/XWEmUWa2Vwz+6SIdcH/TpxzYf+DN7BuJXAOEAPMB5oct80vgZcCj68H3vW77rM4lkHA837XWoxj6Qa0ARadYP1FwGeAAR2BGX7XfBbH0gP4xO86i3EctYA2gceJwA9F/P8VEt9LMY8lVL4XAyoGHkcDM4COx21T5n+HFfM4QuL3V6F6HwBGFvX/UWl8J+WlJR5OU8AW51hCgnNuMt5VCSdyGfCG83wLVDKzWqVT3ekpxrGEBOfcRufcnMDjPOB7fjrTYkh8L8U8lpAQ+Kx3B55GB36OH9BU5n+HFfM4QoaZpQG/AF49wSZB/07KS4iH0xSwxTkWgKsCXZ2jzaxOEetDQXGPNVR0CnQjfmZmTf0u5lQCXX+t8VpLhYXc93KSY4EQ+V4C3bbzgC3Al865E34vZfl3WDGOA0Ln99czwG+AghOsD/p3Ul5CvLz5GEh3zrUAvuTHvwTFP3OAes65lsBzwIc+13NSZlYReB+4zzm3y+96zsYpjiVkvhfnXL5zrhXe7JftzayZ3zWdiWIcR0j8/jKzi4EtzrnZftZRXkL8dKaAxU4yBWwZcMpjcc5td84dCDx9FWhbSrWVtOJ8byHBObfrSDei8+ZPiDazqj6XVSQzi8YLvbeccx8UsUnIfC+nOpZQ+l6OcM7lABOAPsetCpXfYcCJjyOEfn9lApf+//buH0TOIozj+PfHYXEQiGJCDIRwhanERkRC7AI2KdJEiOIfFKsrNJUE09ikskgRFESREFACNoJYiJKACNqqQbQQuUKIkBQGQoKY8Fi8c2ZdcmY1bu7m3u+nubnZl2WGYefZeefdeZKsMGxr7k/y/tQ1cx+TsQTxzXQE7G37z+l3ngAAAn1JREFUMrU/eZBhL7BHHwPPt6eh9wKXq+rCejfqv0jywOpeWJLHGD57G26CbW18D/ihqk6scVkX4zJLXzoal+1J7m3lReAJ4Mepyzb8HDZLP3qZv6rqtaraVVVLDPPwuap6duqyuY9JF1nM7lRtoiNgZ+zLK0kOAtcZ+vLCujX4HyQ5w/B08LYkvwCvMzzoQlW9zXDa3wHgJ+Aq8OL6tPT2ZujLk8BykuvANeCpjTbBNo8DzwHn274lwDFgN3Q3LrP0pZdx2QmcTrLA8EXjw6r6pMM5bJZ+dDF/reVuj4kntkmS1Kmx3E6XJGnTMYhLktQpg7gkSZ0yiEuS1CmDuCRJnTKISyOT5MZEhqhvcotMeHfw3ktZI5ObpP/fKH4nLulvrrVjLyV1zpW4JACSrCR5I8n5lvP5wVa/lORcS0hxNsnuVr8jyUcteci3Sfa1t1pI8m6GfNGftZO5JM2BQVwan8Wp2+mHJ167XFUPA28yZGiCITHI6ZaQ4gPgZKs/CXzRkoc8Anzf6vcAb1XVQ8BvwKE590caLU9sk0YmyZWq2nKL+hVgf1X93BKH/FpV9ye5BOysqj9a/YWq2pbkIrBrIlnFasrPz6tqT/v/KHBPVR2ff8+k8XElLmlSrVH+N36fKN/AZ2+kuTGIS5p0eOLv1638FTcTNzwDfNnKZ4FlgCQLSbberUZKGvgNWRqfxYmsXgCfVtXqz8zuS/Idw2r66Vb3MnAqyavARW5mLTsCvJPkJYYV9zKw4VKSSpuZe+KSgL/2xB+tqkvr3RZJs/F2uiRJnXIlLklSp1yJS5LUKYO4JEmdMohLktQpg7gkSZ0yiEuS1CmDuCRJnfoTgYQM/3NFNakAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtNHcdpSqiEP"
      },
      "source": [
        "exit()"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628543205227
        },
        "id": "nOJ01_qCztqC"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  tweet_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"tweet_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      tweet_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return tweet_texts, predictions, prediction_probs, real_values"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628543218167
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiBUsFAiztqC",
        "outputId": "694729f5-76e1-498c-a2fd-fed5f64cd18d"
      },
      "source": [
        "y_tweet_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628543488243
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4846MMi3ztqD",
        "outputId": "ef40a1ca-f2f1-4f61-fd8f-4ad51ddb073e"
      },
      "source": [
        "print(classification_report(y_test, y_pred,target_names=class_names))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.67      0.69      0.68       118\n",
            "           0       0.66      0.64      0.65       112\n",
            "\n",
            "    accuracy                           0.67       230\n",
            "   macro avg       0.66      0.66      0.66       230\n",
            "weighted avg       0.67      0.67      0.67       230\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628543503220
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "1EF_wkj9ztqD",
        "outputId": "fa39ba26-c2c4-42a7-fa61-690d2036959d"
      },
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('True sentiment')\n",
        "  plt.xlabel('Predicted sentiment');\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628543511691
        },
        "id": "_xjLqo2mztqE"
      },
      "source": [
        "idx = 2\n",
        "\n",
        "tweet_text = y_tweet_texts[idx]\n",
        "true_sentiment = y_test[idx]\n",
        "pred_df = pd.DataFrame({\n",
        "  'class_names': class_names,\n",
        "  'values': y_pred_probs[idx]\n",
        "})"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628543513717
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0xuGj36ztqE",
        "outputId": "6ee8e90e-27df-4731-aa00-1d50de648fff"
      },
      "source": [
        "print(\"\\n\".join(wrap(tweet_text)))\n",
        "print()\n",
        "print(f'True sentiment: {class_names[true_sentiment]}')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "why is paper so sharp someone should put a ban on paper\n",
            "liberalmentality\n",
            "\n",
            "True sentiment: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628543650153
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "YsmlzNc6ztqF",
        "outputId": "ee8fb09f-6719-4d6c-dc8c-46acafe32eeb"
      },
      "source": [
        "sns.barplot(x='values', y='class_names', data=pred_df, orient='h')\n",
        "plt.ylabel('sentiment')\n",
        "plt.xlabel('probability')\n",
        "plt.xlim([0, 1]);"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628279937778
        },
        "id": "Hvi_vCLqztqF"
      },
      "source": [
        "review_text = \"so missing the girls basketball game is just great 😭\""
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628543657871
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZsjnnpxztqG",
        "outputId": "8b6cdfa0-efe9-4e62-a1dd-815ed558ab13"
      },
      "source": [
        "encoded_review = tokenizer.encode_plus(\n",
        "  review_text,\n",
        "  max_length=MAX_LEN,\n",
        "  add_special_tokens=True,\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',\n",
        ")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1628543661688
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY99Dxm_ztqG",
        "outputId": "3cb6541a-e8cc-4848-cfdc-1944c6a20ccc"
      },
      "source": [
        "input_ids = encoded_review['input_ids'].to(device)\n",
        "attention_mask = encoded_review['attention_mask'].to(device)\n",
        "\n",
        "output = model(input_ids, attention_mask)\n",
        "_, prediction = torch.max(output, dim=1)\n",
        "\n",
        "print(f'Tweet text: {review_text}')\n",
        "print(f'Sentiment  : {class_names[prediction]}')"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet text: so missing the girls basketball game is just great 😭\n",
            "Sentiment  : 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "d7iMBomJztqH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}